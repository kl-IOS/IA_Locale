---
title: "Guide Technique D√©taill√© : Cr√©er une IA Locale de A √† Z"
subtitle: "Manuel Complet avec Commandes et Code Pr√™t √† l'Emploi"
author: "Documentation Technique IA Locale"
date: "Janvier 2025"
version: "2.0"
toc: true
toc-depth: 3
---

\newpage

# Glossaire des Termes et Acronymes

## A

**AI / IA (Intelligence Artificielle)** : Capacit√© d'un syst√®me informatique √† effectuer des t√¢ches qui n√©cessitent normalement l'intelligence humaine (raisonnement, apprentissage, compr√©hension du langage).

**API (Application Programming Interface)** : Interface permettant √† des applications de communiquer entre elles via des protocoles standardis√©s.

**ASR (Automatic Speech Recognition)** : Reconnaissance automatique de la parole, conversion audio ‚Üí texte.

## B

**Batch Size** : Nombre d'exemples trait√©s simultan√©ment lors de l'entra√Ænement d'un mod√®le.

**bitsandbytes** : Biblioth√®que Python pour la quantification de mod√®les en 4 et 8 bits, r√©duisant drastiquement l'utilisation m√©moire.

## C

**Chunking** : Processus de segmentation de documents longs en morceaux (chunks) plus petits pour faciliter le traitement par l'IA.

**CUDA (Compute Unified Device Architecture)** : Plateforme de calcul parall√®le de NVIDIA pour exploiter la puissance des GPU.

**Cross-Encoder** : Type de mod√®le qui traite simultan√©ment requ√™te + document pour calculer un score de pertinence (plus pr√©cis mais plus lent que bi-encoder).

## D

**DVC (Data Version Control)** : Syst√®me de gestion de versions pour les donn√©es et mod√®les de machine learning, √©quivalent de Git pour les donn√©es.

## E

**Embeddings** : Repr√©sentations vectorielles denses (tableaux de nombres r√©els) de textes ou autres donn√©es, capturant leur sens s√©mantique dans un espace multidimensionnel.

**Epoch** : Une passe compl√®te sur l'ensemble des donn√©es d'entra√Ænement.

## F

**FAISS (Facebook AI Similarity Search)** : Biblioth√®que ultra-efficace pour la recherche de similarit√© et le clustering de vecteurs denses, optimis√©e CPU et GPU.

**Fine-tuning** : Processus d'ajustement d'un mod√®le pr√©-entra√Æn√© sur des donn√©es sp√©cifiques √† une t√¢che ou un domaine particulier.

## G

**GGUF (GPT-Generated Unified Format)** : Format de fichier pour stocker des mod√®les de langage quantifi√©s, utilis√© par llama.cpp et Ollama.

**GPU (Graphics Processing Unit)** : Processeur graphique utilis√© pour acc√©l√©rer massivement les calculs parall√®les en machine learning.

**Gradient Accumulation** : Technique pour simuler un grand batch size en accumulant les gradients sur plusieurs mini-batches.

## H

**HNSW (Hierarchical Navigable Small World)** : Algorithme de graphe pour la recherche approximative de plus proches voisins, tr√®s performant.

**Hugging Face** : Plateforme et √©cosyst√®me open-source pour le machine learning (mod√®les, datasets, biblioth√®ques).

## K

**kNN (k-Nearest Neighbors)** : Algorithme de recherche des k voisins les plus proches dans un espace vectoriel.

**KV-Cache (Key-Value Cache)** : M√©canisme d'optimisation pour stocker les cl√©s et valeurs calcul√©es dans les transformers, acc√©l√©rant la g√©n√©ration.

## L

**LLM (Large Language Model)** : Mod√®le de langage de grande taille pr√©-entra√Æn√© sur de vastes corpus de texte (billions de mots), capable de comprendre et g√©n√©rer du texte.

**LoRA (Low-Rank Adaptation)** : Technique d'adaptation de mod√®les par ajout de matrices de rang faible, permettant un fine-tuning efficace avec peu de param√®tres.

**Learning Rate** : Taux d'apprentissage, contr√¥le la vitesse de mise √† jour des param√®tres lors de l'entra√Ænement.

## M

**MLOps (Machine Learning Operations)** : Ensemble de pratiques pour d√©ployer et maintenir des mod√®les de machine learning en production.

**MTEB (Massive Text Embedding Benchmark)** : Benchmark de r√©f√©rence pour √©valuer la qualit√© des embeddings de texte sur 58 datasets.

## N

**NLP (Natural Language Processing)** : Traitement automatique du langage naturel, branche de l'IA d√©di√©e √† la compr√©hension et g√©n√©ration de texte.

## P

**PEFT (Parameter-Efficient Fine-Tuning)** : Famille de techniques de fine-tuning qui modifient seulement une petite partie des param√®tres (LoRA, Adapters, Prefix Tuning).

**PII (Personally Identifiable Information)** : Informations permettant d'identifier une personne (nom, email, t√©l√©phone, adresse, num√©ro de s√©curit√© sociale, etc.).

**POC (Proof of Concept)** : Preuve de concept, prototype d√©montrant la faisabilit√© technique et la valeur d'une solution.

**Prompt** : Instruction ou question fournie √† un mod√®le de langage pour guider sa g√©n√©ration de texte.

## Q

**QLoRA (Quantized Low-Rank Adaptation)** : Combinaison de quantification 4-bit et LoRA pour r√©duire drastiquement l'utilisation m√©moire lors du fine-tuning.

**Quantification** : Processus de r√©duction de la pr√©cision des poids d'un mod√®le (ex: de 32 bits √† 8, 4 ou 2 bits) pour √©conomiser m√©moire et calculs.

## R

**RAG (Retrieval-Augmented Generation)** : Approche combinant recherche documentaire (retrieval) et g√©n√©ration de texte, permettant au LLM de s'appuyer sur une base de connaissances externe.

**RAM (Random Access Memory)** : M√©moire vive de l'ordinateur, utilis√©e pour stocker les donn√©es en cours de traitement.

**Reranking** : Processus de reclassement de r√©sultats de recherche pour am√©liorer leur pertinence, utilisant g√©n√©ralement un cross-encoder.

**RGPD (R√®glement G√©n√©ral sur la Protection des Donn√©es)** : R√©glementation europ√©enne sur la protection des donn√©es personnelles, en vigueur depuis 2018.

**ROCm (Radeon Open Compute)** : Plateforme de calcul GPU open-source d'AMD, alternative √† CUDA.

## T

**Tokenizer** : Outil qui d√©coupe un texte en unit√©s (tokens) pour le traitement par un mod√®le.

**Transformer** : Architecture de r√©seau de neurones bas√©e sur le m√©canisme d'attention, fondement des LLM modernes (GPT, BERT, Llama).

**TTS (Text-to-Speech)** : Synth√®se vocale, conversion de texte en parole.

## V

**ViT (Vision Transformer)** : Architecture transformer adapt√©e aux t√¢ches de vision par ordinateur.

**VRAM (Video RAM)** : M√©moire d√©di√©e de la carte graphique, cruciale pour l'entra√Ænement et l'inf√©rence de mod√®les sur GPU.

## Y

**YOLO (You Only Look Once)** : Famille d'algorithmes de d√©tection d'objets en temps r√©el, tr√®s performants pour la vision par ordinateur.

\newpage

# Table des Mati√®res D√©taill√©e

1. [Introduction et Objectifs](#introduction)
2. [Pr√©requis Mat√©riels et Logiciels](#prerequis)
3. [Installation et Configuration de l'Environnement](#installation)
4. [D√©finition du Probl√®me et Choix de l'Approche](#definition-probleme)
5. [Pr√©paration et Nettoyage des Donn√©es](#preparation-donnees)
6. [Algorithmes Cl√©s : RAG, LoRA, Embeddings](#algorithmes)
7. [Impl√©mentation Compl√®te d'un Syst√®me RAG](#implementation-rag)
8. [Fine-tuning avec QLoRA](#fine-tuning)
9. [D√©ploiement d'une API Locale](#deploiement-api)
10. [√âvaluation et Optimisation](#evaluation)
11. [S√©curit√©, Anonymisation et Conformit√© RGPD](#securite)
12. [Scripts Complets Pr√™ts √† l'Emploi](#scripts-complets)
13. [Troubleshooting et FAQ](#troubleshooting)
14. [Ressources et R√©f√©rences](#ressources)

\newpage

# 1. Introduction et Objectifs {#introduction}

## 1.1 Pourquoi une IA Locale ?

Ce guide complet vous accompagne dans la cr√©ation d'un syst√®me d'**Intelligence Artificielle enti√®rement local**, fonctionnant sur votre propre infrastructure sans d√©pendre de services cloud externes.

### Avantages d'une IA Locale

**Confidentialit√© Maximale**
- Vos donn√©es sensibles ne quittent **jamais** votre infrastructure
- Aucune transmission vers des serveurs tiers
- Contr√¥le total sur le traitement des informations

**Ma√Ætrise des Co√ªts**
- √âlimination des frais d'abonnement r√©currents (souvent $20-100/mois par utilisateur)
- Investissement initial amorti sur 2-3 ans
- Pas de facturation √† l'usage

**Souverainet√© Technologique**
- Ind√©pendance vis-√†-vis des fournisseurs externes
- Pas de risque de changement de tarification
- Pas de d√©pendance √† une disponibilit√© de service externe

**Conformit√© RGPD**
- Contr√¥le total sur le stockage et le traitement des donn√©es personnelles
- Tra√ßabilit√© compl√®te
- Facilite les audits de conformit√©

**Personnalisation**
- Adaptation compl√®te aux besoins sp√©cifiques
- Fine-tuning sur vos propres donn√©es
- Aucune limitation d'utilisation

## 1.2 Cas d'Usage Couverts

Ce guide permet d'impl√©menter :

1. **Assistant conversationnel interne** (Q&A sur documentation)
2. **Syst√®me de recherche s√©mantique** (base de connaissances)
3. **R√©sumeur de documents** (rapports, emails, tickets)
4. **Extracteur d'informations** (analyse de contrats, CVs)
5. **G√©n√©rateur de contenu** (emails, descriptions produits)
6. **Analyseur de sentiment** (feedbacks clients)

## 1.3 Technologies Utilis√©es

- **Python 3.10/3.11** : langage principal
- **PyTorch** : framework de deep learning
- **Hugging Face Transformers** : mod√®les pr√©-entra√Æn√©s
- **FAISS / Chroma** : bases de donn√©es vectorielles
- **Ollama / llama.cpp** : inf√©rence LLM locale
- **LangChain** : framework RAG
- **FastAPI** : serveur API
- **Pandoc** : conversion de documents

\newpage

# 2. Pr√©requis Mat√©riels et Logiciels {#prerequis}

## 2.1 Configuration Mat√©rielle

### Configuration Minimale (RAG Simple - CPU uniquement)

```
Processeur : Intel Core i5/i7 ou AMD Ryzen 5/7 (4+ c≈ìurs)
RAM : 16 Go minimum
Stockage : SSD 256 Go minimum
OS : Windows 10/11, macOS 11+, ou Linux (Ubuntu 20.04+)
```

**Cas d'usage** : RAG avec mod√®les 7B quantifi√©s, embeddings l√©gers, corpus < 10 000 documents

### Configuration Recommand√©e (RAG + Fine-tuning L√©ger)

```
Processeur : Intel Core i7/i9 ou AMD Ryzen 7/9 (8+ c≈ìurs)
RAM : 32 Go
GPU : NVIDIA RTX 3060 (12 Go VRAM) ou sup√©rieur
Stockage : SSD NVMe 512 Go minimum
OS : Ubuntu 22.04 LTS (meilleur support GPU)
```

**Cas d'usage** : RAG avanc√©, fine-tuning QLoRA mod√®les 7-13B, corpus < 100 000 documents

### Configuration Optimale (Production)

```
Processeur : Intel Xeon ou AMD EPYC (16+ c≈ìurs)
RAM : 64-128 Go
GPU : NVIDIA RTX 4090 (24 Go VRAM) ou A100 (40-80 Go VRAM)
Stockage : SSD NVMe 1 To+
OS : Ubuntu 22.04 LTS Server
```

**Cas d'usage** : Fine-tuning mod√®les 70B+, RAG temps r√©el, multi-utilisateurs

### Choix du GPU

| GPU | VRAM | Mod√®les Support√©s | Prix Approx. | Recommandation |
|-----|------|-------------------|--------------|----------------|
| RTX 3060 | 12 Go | 7B quantifi√©s | 300-400‚Ç¨ | D√©butant |
| RTX 3090 | 24 Go | 13B, 7B fine-tuning | 1000-1200‚Ç¨ | Bon rapport qualit√©/prix |
| RTX 4090 | 24 Go | 13B fine-tuning, 70B inf√©rence | 1800-2000‚Ç¨ | Haute performance |
| A100 | 40-80 Go | 70B fine-tuning | 10000‚Ç¨+ | Professionnel |

**Apple Silicon (M1/M2/M3)** :
- M1/M2 Pro (16 Go) : √©quivalent RTX 3060
- M1/M2 Max (32-64 Go) : √©quivalent RTX 3090
- M1/M2 Ultra (128 Go) : √©quivalent A100

## 2.2 Logiciels Requis

### Syst√®me d'Exploitation

**Linux (Recommand√© pour GPU)**
```bash
# Ubuntu 22.04 LTS recommand√©
lsb_release -a
# Description: Ubuntu 22.04.3 LTS
```

**macOS (Excellent pour Apple Silicon)**
```bash
# macOS 13+ (Ventura) recommand√© pour Metal Performance
sw_vers
# ProductVersion: 13.0
```

**Windows (Possible mais plus complexe pour GPU)**
```powershell
# Windows 10/11 avec WSL2 recommand√©
wsl --version
# Version WSL : 2.0.0.0
```

### Python

```bash
# Version 3.10 ou 3.11 (√©viter 3.12 pour compatibilit√©)
python --version
# Python 3.11.7
```

**Installation Python (si n√©cessaire)** :

```bash
# Linux (Ubuntu/Debian)
sudo apt update
sudo apt install python3.11 python3.11-venv python3.11-dev

# macOS (avec Homebrew)
brew install python@3.11

# V√©rification
python3.11 --version
```

### Pilotes et Toolkits GPU

#### NVIDIA (Linux/Windows)

**1. V√©rifier le GPU**
```bash
# Lister les GPU NVIDIA
lspci | grep -i nvidia
# 01:00.0 VGA compatible controller: NVIDIA Corporation Device...
```

**2. Installer les pilotes NVIDIA**
```bash
# Ubuntu - m√©thode recommand√©e
sudo ubuntu-drivers devices
sudo ubuntu-drivers autoinstall

# Ou installation manuelle d'une version sp√©cifique
sudo apt install nvidia-driver-535

# Red√©marrer
sudo reboot

# V√©rifier l'installation
nvidia-smi
```

**Sortie attendue de `nvidia-smi`** :
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03   Driver Version: 535.129.03   CUDA Version: 12.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| 30%   45C    P8    15W / 350W |    523MiB / 24576MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

**3. Installer CUDA Toolkit**
```bash
# T√©l√©charger depuis https://developer.nvidia.com/cuda-downloads
# Ou via apt (Ubuntu)
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
sudo dpkg -i cuda-keyring_1.0-1_all.deb
sudo apt update
sudo apt install cuda-toolkit-12-1

# Ajouter au PATH
echo 'export PATH=/usr/local/cuda-12.1/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# V√©rifier
nvcc --version
# Cuda compilation tools, release 12.1, V12.1.105
```

**4. Installer cuDNN**
```bash
# T√©l√©charger depuis https://developer.nvidia.com/cudnn
# N√©cessite un compte NVIDIA (gratuit)

# Installation (exemple pour cuDNN 8.9)
tar -xvf cudnn-linux-x86_64-8.9.0.131_cuda12-archive.tar.xz
sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include
sudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*

# V√©rifier
cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2
```

#### AMD (Linux uniquement)

```bash
# Installer ROCm (Ubuntu 22.04)
sudo apt update
wget https://repo.radeon.com/amdgpu-install/latest/ubuntu/jammy/amdgpu-install_5.7.50700-1_all.deb
sudo apt install ./amdgpu-install_5.7.50700-1_all.deb
sudo amdgpu-install --usecase=rocm

# Ajouter l'utilisateur au groupe
sudo usermod -a -G render,video $LOGNAME

# Red√©marrer
sudo reboot

# V√©rifier
rocm-smi
```

#### Apple Silicon (macOS)

```bash
# Installer Xcode Command Line Tools
xcode-select --install

# V√©rifier
xcode-select -p
# /Library/Developer/CommandLineTools

# Metal est int√©gr√©, pas d'installation suppl√©mentaire
```

### Gestionnaires d'Environnement

#### Conda (Recommand√©)

```bash
# T√©l√©charger Miniconda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

# Installer
bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3

# Initialiser
~/miniconda3/bin/conda init bash
source ~/.bashrc

# V√©rifier
conda --version
# conda 23.11.0

# D√©sactiver l'activation automatique de base
conda config --set auto_activate_base false
```

#### Alternative : venv (Int√©gr√© √† Python)

```bash
# Cr√©er un environnement virtuel
python3.11 -m venv ~/envs/ia-locale

# Activer
source ~/envs/ia-locale/bin/activate

# V√©rifier
which python
# /home/user/envs/ia-locale/bin/python
```

\newpage

# 3. Installation et Configuration de l'Environnement {#installation}

## 3.1 Cr√©ation de l'Environnement

### Option A : Avec Conda (Recommand√©)

```bash
# Cr√©er l'environnement avec Python 3.11
conda create -n ia-locale python=3.11 -y

# Activer l'environnement
conda activate ia-locale

# V√©rifier
python --version
# Python 3.11.7

which python
# /home/user/miniconda3/envs/ia-locale/bin/python
```

### Option B : Avec venv

```bash
# Cr√©er l'environnement
python3.11 -m venv ~/.venvs/ia-locale

# Activer (Linux/macOS)
source ~/.venvs/ia-locale/bin/activate

# Activer (Windows)
.\.venvs\ia-locale\Scripts\activate

# V√©rifier
python --version
which python
```

## 3.2 Installation de PyTorch

PyTorch est le framework fondamental. L'installation d√©pend de votre configuration mat√©rielle.

### Pour GPU NVIDIA (CUDA 12.1)

```bash
# Activer l'environnement
conda activate ia-locale

# Installer PyTorch avec support CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# V√©rifier l'installation
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA disponible: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Aucun\"}')"
```

**Sortie attendue** :
```
PyTorch: 2.1.2+cu121
CUDA disponible: True
GPU: NVIDIA GeForce RTX 3090
```

### Pour CPU uniquement

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA disponible: {torch.cuda.is_available()}')"
```

**Sortie attendue** :
```
PyTorch: 2.1.2+cpu
CUDA disponible: False
```

### Pour Apple Silicon (M1/M2/M3)

```bash
pip install torch torchvision torchaudio

# V√©rifier Metal (GPU Apple)
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'MPS disponible: {torch.backends.mps.is_available()}')"
```

**Sortie attendue** :
```
PyTorch: 2.1.2
MPS disponible: True
```

### Pour AMD GPU (ROCm)

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7

python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'ROCm disponible: {torch.cuda.is_available()}')"
```

## 3.3 Installation des Biblioth√®ques Essentielles

### Biblioth√®ques de Base

```bash
# Activer l'environnement
conda activate ia-locale

# Installer les biblioth√®ques essentielles
pip install transformers accelerate datasets sentencepiece

# V√©rifier
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
# Transformers: 4.36.2
```

### Biblioth√®ques pour RAG

```bash
# Embeddings
pip install sentence-transformers

# Bases vectorielles
pip install faiss-cpu  # ou faiss-gpu si GPU NVIDIA
pip install chromadb

# Framework RAG
pip install langchain langchain-community

# Outils utilitaires
pip install pypdf python-docx python-pptx  # lecture documents
pip install tiktoken  # comptage tokens OpenAI
```

### Biblioth√®ques pour Fine-tuning

```bash
# PEFT (LoRA, QLoRA)
pip install peft

# Quantification 4-bit
pip install bitsandbytes  # N√©cessite CUDA sur Linux

# TRL (Transformer Reinforcement Learning)
pip install trl

# Acc√©l√©ration entra√Ænement
pip install accelerate
```

### Biblioth√®ques pour API et Serveur

```bash
# FastAPI pour serveur API
pip install fastapi uvicorn python-multipart

# Clients HTTP
pip install requests httpx

# Validation de donn√©es
pip install pydantic
```

### Biblioth√®ques Utilitaires

```bash
# Manipulation de donn√©es
pip install pandas numpy scipy

# Visualisation
pip install matplotlib seaborn

# Progress bars
pip install tqdm

# Configuration
pip install pyyaml python-dotenv

# Logging avanc√©
pip install loguru
```

## 3.4 Installation d'Ollama (Serveur LLM Local)

Ollama est la solution la plus simple pour ex√©cuter des LLM localement.

### Linux

```bash
# Installation en une commande
curl -fsSL https://ollama.com/install.sh | sh

# V√©rifier l'installation
ollama --version
# ollama version is 0.1.17

# D√©marrer le serveur (en arri√®re-plan)
ollama serve &

# V√©rifier que le serveur est actif
curl http://localhost:11434/api/tags
```

### macOS

```bash
# Avec Homebrew
brew install ollama

# Ou t√©l√©charger depuis https://ollama.com/download

# Lancer Ollama (d√©marre automatiquement le serveur)
ollama --version
```

### Windows

```powershell
# T√©l√©charger l'installeur depuis https://ollama.com/download
# Installer en suivant les instructions

# V√©rifier
ollama --version
```

### T√©l√©charger un Mod√®le

```bash
# Llama 3.1 8B (recommand√© pour commencer)
ollama pull llama3.1:8b

# V√©rifier les mod√®les install√©s
ollama list

# NAME                    ID              SIZE      MODIFIED
# llama3.1:8b            42182419e950    4.7 GB    2 minutes ago

# Tester le mod√®le
ollama run llama3.1:8b "Bonjour, qui es-tu ?"
```

**Autres mod√®les utiles** :
```bash
# Mod√®les g√©n√©raux
ollama pull llama3.1:70b     # Plus puissant mais n√©cessite 40+ Go VRAM
ollama pull mistral:7b       # Excellent rapport qualit√©/taille
ollama pull phi3:mini        # Tr√®s petit (3.8B) mais performant

# Mod√®les sp√©cialis√©s
ollama pull codellama:7b     # Optimis√© pour le code
ollama pull llama3.1:8b-instruct  # Optimis√© pour instructions

# Lister tous les mod√®les disponibles
curl https://ollama.com/library | grep -o 'href="/library/[^"]*"' | cut -d'"' -f2
```

## 3.5 V√©rification Compl√®te de l'Installation

Cr√©ons un script de v√©rification pour s'assurer que tout fonctionne.

```bash
# Cr√©er le fichier de v√©rification
cat > verif_installation.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de v√©rification de l'installation compl√®te.
"""

import sys

def verifier_bibliotheque(nom, import_nom=None):
    """V√©rifie qu'une biblioth√®que est install√©e."""
    import_nom = import_nom or nom
    try:
        module = __import__(import_nom)
        version = getattr(module, '__version__', 'version inconnue')
        print(f"‚úì {nom}: {version}")
        return True
    except ImportError:
        print(f"‚úó {nom}: NON INSTALL√â")
        return False

def verifier_gpu():
    """V√©rifie la disponibilit√© du GPU."""
    try:
        import torch
        if torch.cuda.is_available():
            print(f"\n‚úì GPU NVIDIA d√©tect√©: {torch.cuda.get_device_name(0)}")
            print(f"  VRAM disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} Go")
            print(f"  CUDA version: {torch.version.cuda}")
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print(f"\n‚úì GPU Apple Silicon (MPS) d√©tect√©")
            return "mps"
        else:
            print(f"\n‚ö† Aucun GPU d√©tect√©, utilisation du CPU")
            return "cpu"
    except Exception as e:
        print(f"\n‚úó Erreur lors de la v√©rification GPU: {e}")
        return None

def verifier_ollama():
    """V√©rifie qu'Ollama est accessible."""
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            modeles = response.json().get('models', [])
            print(f"\n‚úì Ollama est actif")
            print(f"  Mod√®les install√©s: {len(modeles)}")
            for model in modeles[:3]:  # Afficher max 3 mod√®les
                print(f"    - {model['name']}")
            return True
        else:
            print(f"\n‚úó Ollama: erreur HTTP {response.status_code}")
            return False
    except requests.exceptions.RequestException:
        print(f"\n‚úó Ollama n'est pas accessible (serveur non d√©marr√©?)")
        print(f"  Lancer: ollama serve")
        return False
    except Exception as e:
        print(f"\n‚úó Erreur Ollama: {e}")
        return False

def main():
    """Fonction principale."""
    print("=" * 60)
    print("V√âRIFICATION DE L'INSTALLATION - IA LOCALE")
    print("=" * 60)

    print("\nüì¶ BIBLIOTH√àQUES PYTHON:")
    bibliotheques = [
        ('PyTorch', 'torch'),
        ('Transformers', 'transformers'),
        ('Sentence-Transformers', 'sentence_transformers'),
        ('FAISS', 'faiss'),
        ('ChromaDB', 'chromadb'),
        ('LangChain', 'langchain'),
        ('PEFT', 'peft'),
        ('BitsAndBytes', 'bitsandbytes'),
        ('FastAPI', 'fastapi'),
        ('Requests', 'requests'),
    ]

    resultats = {}
    for nom, import_nom in bibliotheques:
        resultats[nom] = verifier_bibliotheque(nom, import_nom)

    print("\nüñ•Ô∏è  MAT√âRIEL:")
    device = verifier_gpu()

    print("\nü§ñ SERVEUR LLM:")
    ollama_ok = verifier_ollama()

    print("\n" + "=" * 60)
    print("R√âSUM√â")
    print("=" * 60)

    nb_ok = sum(resultats.values())
    nb_total = len(bibliotheques)

    print(f"\nBiblioth√®ques: {nb_ok}/{nb_total} install√©es")
    print(f"GPU: {device.upper() if device else 'NON D√âTECT√â'}")
    print(f"Ollama: {'‚úì OK' if ollama_ok else '‚úó NON ACCESSIBLE'}")

    if nb_ok == nb_total and device and ollama_ok:
        print("\nüéâ Installation compl√®te et fonctionnelle !")
        return 0
    else:
        print("\n‚ö†Ô∏è  Installation incompl√®te, v√©rifier les erreurs ci-dessus")
        return 1

if __name__ == "__main__":
    sys.exit(main())
EOF

# Rendre ex√©cutable
chmod +x verif_installation.py

# Ex√©cuter
python verif_installation.py
```

**Sortie attendue (installation compl√®te)** :
```
============================================================
V√âRIFICATION DE L'INSTALLATION - IA LOCALE
============================================================

üì¶ BIBLIOTH√àQUES PYTHON:
‚úì PyTorch: 2.1.2+cu121
‚úì Transformers: 4.36.2
‚úì Sentence-Transformers: 2.2.2
‚úì FAISS: 1.7.4
‚úì ChromaDB: 0.4.18
‚úì LangChain: 0.1.0
‚úì PEFT: 0.7.1
‚úì BitsAndBytes: 0.41.3
‚úì FastAPI: 0.108.0
‚úì Requests: 2.31.0

üñ•Ô∏è  MAT√âRIEL:

‚úì GPU NVIDIA d√©tect√©: NVIDIA GeForce RTX 3090
  VRAM disponible: 24.00 Go
  CUDA version: 12.1

ü§ñ SERVEUR LLM:

‚úì Ollama est actif
  Mod√®les install√©s: 2
    - llama3.1:8b
    - mistral:7b

============================================================
R√âSUM√â
============================================================

Biblioth√®ques: 10/10 install√©es
GPU: CUDA
Ollama: ‚úì OK

üéâ Installation compl√®te et fonctionnelle !
```

## 3.6 Structure de Projet Recommand√©e

Cr√©ons la structure de dossiers pour le projet.

```bash
# Cr√©er la structure
mkdir -p ~/projets/ia-locale
cd ~/projets/ia-locale

# Cr√©er les sous-dossiers
mkdir -p {data/{raw,processed,chunks},models/{embeddings,llm,adapters},index/{faiss,chroma},scripts,tests,config,logs,docs}

# Cr√©er les fichiers de base
touch config/config.yaml
touch scripts/__init__.py
touch tests/__init__.py
touch README.md
touch requirements.txt
touch .gitignore

# Structure finale
tree -L 2
```

**Structure** :
```
ia-locale/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml          # Configuration centrale
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                 # Donn√©es brutes
‚îÇ   ‚îú‚îÄ‚îÄ processed/           # Donn√©es nettoy√©es
‚îÇ   ‚îî‚îÄ‚îÄ chunks/              # Documents segment√©s
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/          # Mod√®les d'embeddings
‚îÇ   ‚îú‚îÄ‚îÄ llm/                 # LLMs locaux
‚îÇ   ‚îî‚îÄ‚îÄ adapters/            # Adaptateurs LoRA
‚îú‚îÄ‚îÄ index/
‚îÇ   ‚îú‚îÄ‚îÄ faiss/               # Index FAISS
‚îÇ   ‚îî‚îÄ‚îÄ chroma/              # Base Chroma
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ 01_preparation_donnees.py
‚îÇ   ‚îú‚îÄ‚îÄ 02_indexation.py
‚îÇ   ‚îú‚îÄ‚îÄ 03_rag.py
‚îÇ   ‚îú‚îÄ‚îÄ 04_fine_tuning.py
‚îÇ   ‚îî‚îÄ‚îÄ 05_api_serveur.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_rag.py
‚îÇ   ‚îî‚îÄ‚îÄ test_qualite.py
‚îú‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ docs/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .gitignore
```

### Fichier requirements.txt

```bash
cat > requirements.txt << 'EOF'
# PyTorch (installer s√©par√©ment avec la bonne version CUDA/CPU/MPS)
# torch>=2.1.0
# torchvision>=0.16.0
# torchaudio>=2.1.0

# Transformers et mod√®les
transformers>=4.36.0
accelerate>=0.25.0
datasets>=2.16.0
sentencepiece>=0.1.99
tokenizers>=0.15.0

# RAG et embeddings
sentence-transformers>=2.2.2
faiss-cpu>=1.7.4  # ou faiss-gpu
chromadb>=0.4.18
langchain>=0.1.0
langchain-community>=0.0.10

# Fine-tuning
peft>=0.7.0
bitsandbytes>=0.41.0
trl>=0.7.0

# API et serveur
fastapi>=0.108.0
uvicorn[standard]>=0.25.0
python-multipart>=0.0.6
requests>=2.31.0
httpx>=0.25.0

# Manipulation de donn√©es
pandas>=2.1.0
numpy>=1.24.0
scipy>=1.11.0

# Lecture de documents
pypdf>=3.17.0
python-docx>=1.1.0
python-pptx>=0.6.23
openpyxl>=3.1.0

# Utilitaires
pyyaml>=6.0.1
python-dotenv>=1.0.0
tqdm>=4.66.0
loguru>=0.7.0

# Validation
pydantic>=2.5.0

# Tests
pytest>=7.4.0
pytest-cov>=4.1.0
EOF
```

### Fichier .gitignore

```bash
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
.venv/

# Donn√©es et mod√®les (trop gros pour git)
data/raw/*
data/processed/*
data/chunks/*
models/embeddings/*
models/llm/*
models/adapters/*
index/faiss/*
index/chroma/*
*.bin
*.safetensors
*.gguf
*.pkl
*.parquet

# Logs
logs/*
*.log

# Configuration locale
.env
config/local.yaml

# IDE
.vscode/
.idea/
*.swp
*.swo
.DS_Store

# Jupyter
.ipynb_checkpoints/
*.ipynb

# Exceptions (fichiers √† garder)
!data/raw/.gitkeep
!data/processed/.gitkeep
!models/embeddings/.gitkeep
!index/faiss/.gitkeep
EOF
```

### Fichier README.md

```bash
cat > README.md << 'EOF'
# Projet IA Locale

Syst√®me d'Intelligence Artificielle local avec RAG et fine-tuning.

## Installation

```bash
# Cr√©er l'environnement
conda create -n ia-locale python=3.11 -y
conda activate ia-locale

# Installer PyTorch (adapter selon votre GPU)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Installer les d√©pendances
pip install -r requirements.txt
```

## D√©marrage Rapide

```bash
# 1. Pr√©parer les donn√©es
python scripts/01_preparation_donnees.py

# 2. Cr√©er l'index vectoriel
python scripts/02_indexation.py

# 3. Lancer le serveur RAG
python scripts/05_api_serveur.py
```

## Documentation

Voir le guide complet dans `docs/guide_technique_detaille.md`
EOF
```

### Configuration de Base

```bash
cat > config/config.yaml << 'EOF'
# Configuration du syst√®me IA Locale

# Chemins
paths:
  data_raw: "data/raw"
  data_processed: "data/processed"
  data_chunks: "data/chunks"
  models: "models"
  index: "index"
  logs: "logs"

# Embeddings
embeddings:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  dimension: 384
  device: "cuda"  # ou "cpu", "mps"
  batch_size: 32

# Base vectorielle
vectordb:
  type: "faiss"  # ou "chroma"
  faiss_index_type: "Flat"  # ou "IVFFlat", "HNSW"
  persist_path: "index/faiss/index.bin"

# LLM
llm:
  provider: "ollama"  # ou "llamacpp", "vllm"
  model_name: "llama3.1:8b"
  api_url: "http://localhost:11434"
  temperature: 0.7
  max_tokens: 512
  top_p: 0.9
  top_k: 40

# Chunking
chunking:
  chunk_size: 1000
  chunk_overlap: 200
  separators: ["\n\n", "\n", ". ", "! ", "? ", " "]

# RAG
rag:
  top_k: 4
  use_reranker: false
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  reranker_top_k: 2

# Fine-tuning
fine_tuning:
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  learning_rate: 2e-4
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4

# API
api:
  host: "0.0.0.0"
  port: 8000
  reload: false
  log_level: "info"

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/ia-locale.log"
EOF
```

## 3.7 Commandes Utiles de Gestion

### Gestion de l'Environnement Conda

```bash
# Lister les environnements
conda env list

# Activer
conda activate ia-locale

# D√©sactiver
conda deactivate

# Exporter l'environnement (pour partager)
conda env export > environment.yml

# Cr√©er depuis un fichier
conda env create -f environment.yml

# Supprimer l'environnement
conda env remove -n ia-locale

# Mettre √† jour conda
conda update conda

# Nettoyer le cache
conda clean --all
```

### Gestion des Packages pip

```bash
# Lister les packages install√©s
pip list

# V√©rifier les packages obsol√®tes
pip list --outdated

# Mettre √† jour un package
pip install --upgrade transformers

# Mettre √† jour tous les packages (attention !)
pip list --outdated --format=freeze | grep -v '^\-e' | cut -d = -f 1  | xargs -n1 pip install -U

# Installer depuis requirements.txt
pip install -r requirements.txt

# G√©n√©rer requirements.txt des packages install√©s
pip freeze > requirements_freeze.txt

# D√©sinstaller un package
pip uninstall transformers

# Voir les infos d'un package
pip show transformers
```

### Gestion d'Ollama

```bash
# Lister les mod√®les install√©s
ollama list

# T√©l√©charger un mod√®le
ollama pull llama3.1:8b

# Supprimer un mod√®le
ollama rm llama3.1:8b

# Afficher les infos d'un mod√®le
ollama show llama3.1:8b

# Copier un mod√®le (cr√©er une variante)
ollama cp llama3.1:8b mon-llama:latest

# Voir les mod√®les disponibles en ligne
curl https://ollama.com/api/tags

# D√©marrer le serveur
ollama serve

# Arr√™ter le serveur
pkill ollama

# Logs du serveur
tail -f ~/.ollama/logs/server.log

# Emplacement des mod√®les
ls -lh ~/.ollama/models/
```

\newpage

# 4. D√©finition du Probl√®me et Choix de l'Approche {#definition-probleme}

## 4.1 Typologie des Probl√®mes d'IA

### Questions √† Se Poser

Avant de choisir une approche technique, il est crucial de bien d√©finir le probl√®me √† r√©soudre.

**1. Quel est l'objectif m√©tier ?**
- R√©pondre √† des questions (Q&A interne)
- R√©sumer des documents
- Extraire des informations structur√©es
- G√©n√©rer du contenu (emails, rapports)
- Analyser du sentiment
- Classifier des documents

**2. Quelles donn√©es ai-je √† disposition ?**
- Volume : < 1 000 docs, 1 000-10 000 docs, > 10 000 docs
- Format : PDF, Word, emails, bases de donn√©es
- Qualit√© : propres, bruit√©es, multilingues
- √âvolution : statiques ou mises √† jour fr√©quentes

**3. Quelles sont mes contraintes ?**
- Latence : temps r√©el (< 1s), acceptable (1-5s), batch
- Confidentialit√© : critique, importante, secondaire
- Budget : limit√©, moyen, √©lev√©
- Comp√©tences : √©quipe ML exp√©riment√©e ou non

## 4.2 Arbre de D√©cision Technique

```mermaid
graph TD
    A[D√©finir le besoin] --> B{Type de t√¢che ?}

    B -->|Q&A sur documents| C{Volume de documents ?}
    B -->|Classification| D{Donn√©es d'entra√Ænement ?}
    B -->|G√©n√©ration| E{Style sp√©cifique requis ?}

    C -->|< 10k docs| F[RAG Simple<br/>FAISS + Ollama]
    C -->|> 10k docs| G[RAG Avanc√©<br/>Qdrant + Reranker]

    D -->|Oui, beaucoup| H[Fine-tuning<br/>LoRA/QLoRA]
    D -->|Non, peu| I[Few-shot Learning<br/>Prompting]

    E -->|Oui| J[Fine-tuning Style<br/>LoRA sur exemples]
    E -->|Non| K[LLM Standard<br/>Prompting]

    style F fill:#27AE60,color:#fff
    style G fill:#F39C12,color:#fff
    style H fill:#E74C3C,color:#fff
```

### Recommandations par Cas d'Usage

| Cas d'Usage | Approche | Complexit√© | Temps | Co√ªt |
|-------------|----------|------------|-------|------|
| **Assistant documentation** | RAG | ‚≠ê‚≠ê | 2-4 sem | 15-25 k‚Ç¨ |
| **R√©sumeur documents** | Prompting LLM | ‚≠ê | 1 sem | 5-10 k‚Ç¨ |
| **Chatbot support client** | RAG + Fine-tuning | ‚≠ê‚≠ê‚≠ê | 6-8 sem | 40-60 k‚Ç¨ |
| **Extracteur d'infos** | Few-shot + Regex | ‚≠ê‚≠ê | 2-3 sem | 10-20 k‚Ç¨ |
| **G√©n√©rateur rapports** | Fine-tuning | ‚≠ê‚≠ê‚≠ê | 4-6 sem | 30-50 k‚Ç¨ |
| **Analyseur sentiment** | Classification | ‚≠ê‚≠ê | 3-4 sem | 15-30 k‚Ç¨ |

## 4.3 Matrice de D√©cision RAG vs Fine-tuning

### Crit√®res de Choix

| Crit√®re | RAG | Fine-tuning |
|---------|-----|-------------|
| **Donn√©es √©voluent souvent** | ‚úÖ Excellent | ‚ùå N√©cessite r√©-entra√Ænement |
| **Besoin de sources cit√©es** | ‚úÖ Natif | ‚ö†Ô∏è Possible mais complexe |
| **Style d'√©criture sp√©cifique** | ‚ö†Ô∏è Limit√© | ‚úÖ Excellent |
| **Donn√©es d'entra√Ænement** | ‚ö†Ô∏è Peu n√©cessaires | ‚ùå Beaucoup (1000+) |
| **Time-to-market** | ‚úÖ Rapide (2-4 sem) | ‚ö†Ô∏è Long (6-8 sem) |
| **Co√ªt** | ‚úÖ Faible (10-25 k‚Ç¨) | ‚ö†Ô∏è √âlev√© (40-80 k‚Ç¨) |
| **Expertise ML requise** | ‚ö†Ô∏è Moyenne | ‚ùå √âlev√©e |
| **Maintenabilit√©** | ‚úÖ Simple | ‚ö†Ô∏è Complexe |

### D√©cision Recommand√©e

```python
# Algorithme de d√©cision simplifi√©
def choisir_approche(cas_usage):
    """
    D√©termine l'approche optimale selon le cas d'usage.

    Args:
        cas_usage: Dictionnaire avec les caract√©ristiques du projet

    Returns:
        Approche recommand√©e
    """
    # V√©rifier si RAG est appropri√©
    if (cas_usage['type'] == 'qa' and
        cas_usage['volume_docs'] > 100 and
        cas_usage['donnees_evoluent']):
        return "RAG"

    # V√©rifier si fine-tuning est n√©cessaire
    if (cas_usage['style_specifique'] and
        cas_usage['exemples_disponibles'] > 1000):
        return "Fine-tuning (LoRA/QLoRA)"

    # V√©rifier si prompting suffit
    if (cas_usage['type'] in ['resume', 'extraction'] and
        cas_usage['volume'] < 1000):
        return "Prompting simple"

    # Par d√©faut : RAG (le plus versatile)
    return "RAG (approche s√ªre)"

# Exemple d'utilisation
projet = {
    'type': 'qa',
    'volume_docs': 5000,
    'donnees_evoluent': True,
    'style_specifique': False,
    'exemples_disponibles': 0
}

print(choisir_approche(projet))
# Output: "RAG"
```

\newpage

# 5. Pr√©paration et Nettoyage des Donn√©es {#preparation-donnees}

## 5.1 Sources de Donn√©es

### Donn√©es Internes

```python
# scripts/collecte_donnees.py
"""
Script de collecte de donn√©es depuis diverses sources internes.
"""

import os
from pathlib import Path
from typing import List, Dict
import shutil

def collecter_pdf(repertoire_source: str, repertoire_dest: str) -> List[Path]:
    """
    Collecte tous les fichiers PDF depuis un r√©pertoire source.

    Args:
        repertoire_source: R√©pertoire contenant les PDFs
        repertoire_dest: R√©pertoire de destination

    Returns:
        Liste des fichiers copi√©s
    """
    source = Path(repertoire_source)
    dest = Path(repertoire_dest)
    dest.mkdir(parents=True, exist_ok=True)

    fichiers_copies = []

    # Parcourir r√©cursivement
    for fichier_pdf in source.rglob("*.pdf"):
        # Cr√©er un nom unique pour √©viter les collisions
        nom_relatif = fichier_pdf.relative_to(source)
        fichier_dest = dest / nom_relatif

        # Cr√©er les sous-r√©pertoires si n√©cessaire
        fichier_dest.parent.mkdir(parents=True, exist_ok=True)

        # Copier le fichier
        shutil.copy2(fichier_pdf, fichier_dest)
        fichiers_copies.append(fichier_dest)

    print(f"‚úì {len(fichiers_copies)} fichiers PDF collect√©s")
    return fichiers_copies

# Exemple d'utilisation
if __name__ == "__main__":
    # Collecter depuis plusieurs sources
    sources = [
        "/chemin/vers/documentation",
        "/chemin/vers/rapports",
        "/chemin/vers/procedures"
    ]

    for source in sources:
        if os.path.exists(source):
            collecter_pdf(source, "data/raw/pdfs")
```

### Lecture et Extraction

```python
# scripts/extraction_texte.py
"""
Extraction de texte depuis diff√©rents formats de fichiers.
"""

from pathlib import Path
from typing import Dict, Optional
import pypdf
from docx import Document
import json


def extraire_pdf(chemin_fichier: str) -> Dict[str, any]:
    """
    Extrait le texte d'un fichier PDF.

    Args:
        chemin_fichier: Chemin vers le PDF

    Returns:
        Dictionnaire avec texte et m√©tadonn√©es
    """
    texte_complet = []
    metadata = {}

    with open(chemin_fichier, 'rb') as f:
        lecteur = pypdf.PdfReader(f)

        # Extraire les m√©tadonn√©es
        if lecteur.metadata:
            metadata = {
                'titre': lecteur.metadata.get('/Title', ''),
                'auteur': lecteur.metadata.get('/Author', ''),
                'sujet': lecteur.metadata.get('/Subject', ''),
                'date_creation': str(lecteur.metadata.get('/CreationDate', ''))
            }

        # Extraire le texte de chaque page
        for numero_page, page in enumerate(lecteur.pages):
            texte_page = page.extract_text()
            if texte_page:
                texte_complet.append(f"--- Page {numero_page + 1} ---\n{texte_page}")

    return {
        'fichier': chemin_fichier,
        'texte': '\n\n'.join(texte_complet),
        'metadata': metadata,
        'nombre_pages': len(lecteur.pages)
    }


def extraire_docx(chemin_fichier: str) -> Dict[str, any]:
    """
    Extrait le texte d'un fichier Word.

    Args:
        chemin_fichier: Chemin vers le DOCX

    Returns:
        Dictionnaire avec texte et m√©tadonn√©es
    """
    doc = Document(chemin_fichier)

    # Extraire les paragraphes
    paragraphes = [para.text for para in doc.paragraphs if para.text.strip()]

    # Extraire les propri√©t√©s du document
    metadata = {
        'titre': doc.core_properties.title or '',
        'auteur': doc.core_properties.author or '',
        'sujet': doc.core_properties.subject or '',
        'date_creation': str(doc.core_properties.created) if doc.core_properties.created else ''
    }

    return {
        'fichier': chemin_fichier,
        'texte': '\n\n'.join(paragraphes),
        'metadata': metadata,
        'nombre_paragraphes': len(paragraphes)
    }


def extraire_texte_fichier(chemin_fichier: str) -> Optional[Dict]:
    """
    Extrait le texte d'un fichier selon son extension.

    Args:
        chemin_fichier: Chemin vers le fichier

    Returns:
        Dictionnaire avec le contenu extrait ou None
    """
    fichier = Path(chemin_fichier)

    if not fichier.exists():
        print(f"‚úó Fichier introuvable: {chemin_fichier}")
        return None

    try:
        if fichier.suffix.lower() == '.pdf':
            return extraire_pdf(str(fichier))
        elif fichier.suffix.lower() in ['.docx', '.doc']:
            return extraire_docx(str(fichier))
        elif fichier.suffix.lower() in ['.txt', '.md']:
            with open(fichier, 'r', encoding='utf-8') as f:
                return {
                    'fichier': str(fichier),
                    'texte': f.read(),
                    'metadata': {},
                    'type': 'texte_brut'
                }
        else:
            print(f"‚ö† Format non support√©: {fichier.suffix}")
            return None

    except Exception as e:
        print(f"‚úó Erreur lors de l'extraction de {fichier.name}: {e}")
        return None


# Traitement batch
def extraire_tous_fichiers(repertoire: str, formats: List[str] = None) -> List[Dict]:
    """
    Extrait le texte de tous les fichiers d'un r√©pertoire.

    Args:
        repertoire: R√©pertoire contenant les fichiers
        formats: Liste des extensions √† traiter (None = tous)

    Returns:
        Liste des contenus extraits
    """
    if formats is None:
        formats = ['.pdf', '.docx', '.txt', '.md']

    dossier = Path(repertoire)
    contenus = []

    for fichier in dossier.rglob("*"):
        if fichier.suffix.lower() in formats and fichier.is_file():
            print(f"Extraction: {fichier.name}...")
            contenu = extraire_texte_fichier(str(fichier))
            if contenu:
                contenus.append(contenu)

    print(f"\n‚úì {len(contenus)} fichiers extraits avec succ√®s")
    return contenus


# Sauvegarde en JSONL
def sauvegarder_jsonl(contenus: List[Dict], fichier_sortie: str):
    """
    Sauvegarde les contenus extraits au format JSONL.

    Args:
        contenus: Liste des contenus
        fichier_sortie: Chemin du fichier de sortie
    """
    with open(fichier_sortie, 'w', encoding='utf-8') as f:
        for contenu in contenus:
            json.dump(contenu, f, ensure_ascii=False)
            f.write('\n')

    print(f"‚úì Contenus sauvegard√©s dans {fichier_sortie}")


if __name__ == "__main__":
    # Extraire tous les documents
    contenus = extraire_tous_fichiers("data/raw")

    # Sauvegarder
    sauvegarder_jsonl(contenus, "data/processed/documents_extraits.jsonl")
```

## 5.2 Nettoyage des Donn√©es

### Script de Nettoyage Complet

```python
# scripts/nettoyage_donnees.py
"""
Nettoyage et normalisation des donn√©es textuelles.
"""

import re
import unicodedata
from typing import List
from difflib import SequenceMatcher


def nettoyer_texte(texte: str) -> str:
    """
    Nettoie un texte en supprimant les √©l√©ments ind√©sirables.

    Args:
        texte: Texte brut √† nettoyer

    Returns:
        Texte nettoy√©
    """
    # Supprimer les balises HTML/XML
    texte = re.sub(r'<[^>]+>', '', texte)

    # Supprimer les URLs
    texte = re.sub(
        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
        '',
        texte
    )

    # Normaliser les espaces multiples
    texte = re.sub(r'\s+', ' ', texte)

    # Supprimer les espaces en d√©but/fin de ligne
    lignes = [ligne.strip() for ligne in texte.split('\n')]
    texte = '\n'.join(ligne for ligne in lignes if ligne)

    # Normaliser les caract√®res Unicode (NFD -> NFC)
    texte = unicodedata.normalize('NFC', texte)

    return texte.strip()


def corriger_erreurs_ocr(texte: str) -> str:
    """
    Corrige les erreurs courantes d'OCR.

    Args:
        texte: Texte issu d'OCR

    Returns:
        Texte corrig√©
    """
    corrections = {
        r'\b0\b': 'O',  # Z√©ro confondu avec O
        r'\bl\b': 'I',  # L minuscule confondu avec I
        r'rn': 'm',     # rn confondu avec m
        r'vv': 'w',     # vv confondu avec w
    }

    for pattern, remplacement in corrections.items():
        texte = re.sub(pattern, remplacement, texte)

    return texte


def dedupliquer_documents(documents: List[str], seuil: float = 0.95) -> List[str]:
    """
    Supprime les documents quasi-identiques.

    Args:
        documents: Liste de documents
        seuil: Seuil de similarit√© (0-1)

    Returns:
        Liste de documents d√©dupliqu√©s
    """
    documents_uniques = []

    for doc in documents:
        est_doublon = False

        for doc_unique in documents_uniques:
            # Calculer la similarit√©
            similarite = SequenceMatcher(None, doc, doc_unique).ratio()

            if similarite > seuil:
                est_doublon = True
                break

        if not est_doublon:
            documents_uniques.append(doc)

    print(f"D√©duplication: {len(documents)} ‚Üí {len(documents_uniques)} documents")
    return documents_uniques


def pipeline_nettoyage(texte: str, corriger_ocr: bool = False) -> str:
    """
    Pipeline complet de nettoyage.

    Args:
        texte: Texte brut
        corriger_ocr: Appliquer les corrections OCR

    Returns:
        Texte nettoy√©
    """
    # 1. Nettoyage de base
    texte = nettoyer_texte(texte)

    # 2. Correction OCR si demand√©
    if corriger_ocr:
        texte = corriger_erreurs_ocr(texte)

    # 3. Suppression lignes vides multiples
    texte = re.sub(r'\n\n+', '\n\n', texte)

    return texte


if __name__ == "__main__":
    # Test
    texte_test = """
    <html><p>Ceci est un    exemple    de texte.</p></html>


    Avec des espaces   multiples et des  lignes vides.
    http://example.com/lien-inutile
    """

    texte_propre = pipeline_nettoyage(texte_test)
    print("Texte nettoy√©:")
    print(texte_propre)
```

**Commandes d'ex√©cution** :

```bash
# Nettoyer tous les documents
python scripts/nettoyage_donnees.py

# Avec logs d√©taill√©s
python scripts/nettoyage_donnees.py --verbose
```

\newpage

*Le guide continue avec les sections suivantes...*

---

## 6. Algorithmes Cl√©s : RAG, LoRA et Embeddings

Cette section explique en d√©tail les algorithmes fondamentaux pour cr√©er une IA locale.

### 6.1 RAG (Retrieval-Augmented Generation)

Le [RAG](#def-rag) combine la recherche de documents avec la g√©n√©ration de texte pour cr√©er des r√©ponses pr√©cises et contextualis√©es.

#### Architecture RAG

```mermaid
graph TB
    A[Question utilisateur] --> B[Cr√©ation embedding question]
    B --> C[Recherche similarit√© vectorielle]
    C --> D[Base de documents vectoris√©s]
    D --> E[Top K documents pertinents]
    E --> F[Construction du contexte]
    F --> G[Prompt augment√©]
    G --> H[LLM Local Ollama]
    H --> I[R√©ponse g√©n√©r√©e]

    style A fill:#5EA8A7
    style I fill:#FE4447
    style H fill:#F4D03F
```

#### Composants du RAG

**1. Embedding Model** : Transforme le texte en vecteurs num√©riques

| Mod√®le | Dimensions | Performance | Taille | Cas d'usage |
|--------|-----------|-------------|--------|-------------|
| **all-MiniLM-L6-v2** | 384 | Rapide ‚ö° | 80 MB | Documents courts, prototypage |
| **all-mpnet-base-v2** | 768 | √âquilibr√© üéØ | 420 MB | Usage g√©n√©ral recommand√© |
| **multilingual-e5-large** | 1024 | Multilingue üåç | 2.24 GB | Documents fran√ßais/multilingues |
| **text-embedding-ada-002** | 1536 | Haute qualit√© üíé | API OpenAI | Production avec API |

**2. Vector Database** : Stocke et recherche les embeddings

| Base vectorielle | Vitesse | Scalabilit√© | Persistance | Complexit√© |
|-----------------|---------|-------------|-------------|------------|
| **FAISS** | ‚ö°‚ö°‚ö° Tr√®s rapide | Millions de vecteurs | Fichiers locaux | Moyenne |
| **Chroma** | ‚ö°‚ö° Rapide | Centaines de milliers | SQLite int√©gr√© | Faible ‚úÖ |
| **Milvus** | ‚ö°‚ö° Rapide | Milliards de vecteurs | Distribu√© | √âlev√©e |
| **Qdrant** | ‚ö°‚ö°‚ö° Tr√®s rapide | Millions de vecteurs | Distribu√© | Moyenne |

**Recommandation** : Pour d√©buter, utilisez **Chroma** (simple) ou **FAISS** (performant).

**3. LLM** : G√©n√®re la r√©ponse finale

#### Code RAG complet avec LangChain

```python
"""
Script complet d'impl√©mentation RAG avec Ollama et Chroma.
N√©cessite : pip install langchain chromadb sentence-transformers ollama
"""

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# √âTAPE 1 : Chargement et d√©coupage des documents
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def charger_et_decouper_documents(repertoire_docs: str):
    """
    Charge tous les documents texte d'un r√©pertoire et les d√©coupe en chunks.

    Args:
        repertoire_docs: Chemin vers le dossier contenant les documents

    Returns:
        Liste de documents d√©coup√©s
    """
    print(f"üìÇ Chargement des documents depuis {repertoire_docs}...")

    # Charger tous les fichiers .txt
    loader = DirectoryLoader(
        repertoire_docs,
        glob="**/*.txt",
        loader_cls=TextLoader,
        loader_kwargs={'encoding': 'utf-8'}
    )
    documents = loader.load()

    print(f"‚úÖ {len(documents)} documents charg√©s")

    # D√©couper en chunks de 500 caract√®res avec overlap de 50
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,        # Taille de chaque morceau
        chunk_overlap=50,      # Chevauchement entre morceaux
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
    )

    chunks = text_splitter.split_documents(documents)
    print(f"‚úÇÔ∏è  Documents d√©coup√©s en {len(chunks)} chunks")

    return chunks


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# √âTAPE 2 : Cr√©ation de la base vectorielle
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def creer_base_vectorielle(chunks, persist_directory: str = "./chroma_db"):
    """
    Cr√©e ou charge une base vectorielle Chroma avec les embeddings.

    Args:
        chunks: Liste de documents d√©coup√©s
        persist_directory: R√©pertoire de persistance Chroma

    Returns:
        Base vectorielle Chroma
    """
    print("üßÆ Cr√©ation des embeddings avec all-mpnet-base-v2...")

    # Mod√®le d'embeddings (fran√ßais + anglais)
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-mpnet-base-v2",
        model_kwargs={'device': 'cpu'},  # Utiliser 'cuda' si GPU disponible
        encode_kwargs={'normalize_embeddings': True}
    )

    # V√©rifier si la base existe d√©j√†
    if os.path.exists(persist_directory):
        print(f"üì¶ Chargement de la base existante depuis {persist_directory}")
        vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings
        )
    else:
        print(f"üî® Cr√©ation de la base vectorielle...")
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=persist_directory
        )
        vectorstore.persist()
        print(f"üíæ Base sauvegard√©e dans {persist_directory}")

    return vectorstore


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# √âTAPE 3 : Configuration du LLM Ollama
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def configurer_llm():
    """
    Configure le mod√®le Ollama local.

    Returns:
        Instance Ollama configur√©e
    """
    print("ü¶ô Configuration d'Ollama (llama3.1:8b)...")

    llm = Ollama(
        model="llama3.1:8b",
        temperature=0.2,  # Faible = r√©ponses plus d√©terministes
        num_ctx=4096,     # Contexte de 4096 tokens
        num_predict=512   # Maximum 512 tokens de r√©ponse
    )

    return llm


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# √âTAPE 4 : Cr√©ation du prompt personnalis√©
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def creer_prompt_template():
    """
    Cr√©e un template de prompt optimis√© pour le RAG.

    Returns:
        PromptTemplate configur√©
    """
    template = """Tu es un assistant IA qui r√©pond uniquement en te basant sur le contexte fourni.

CONTEXTE :
{context}

QUESTION : {question}

INSTRUCTIONS :
- R√©ponds uniquement avec les informations du contexte ci-dessus
- Si la r√©ponse n'est pas dans le contexte, dis "Je n'ai pas trouv√© cette information dans mes documents"
- Sois pr√©cis et concis
- Cite les sources si possible

R√âPONSE :"""

    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )

    return prompt


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# √âTAPE 5 : Assemblage de la cha√Æne RAG
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def creer_chaine_rag(vectorstore, llm, prompt):
    """
    Assemble tous les composants en une cha√Æne RAG compl√®te.

    Args:
        vectorstore: Base vectorielle Chroma
        llm: Mod√®le Ollama
        prompt: Template de prompt

    Returns:
        Cha√Æne RetrievalQA compl√®te
    """
    print("üîó Assemblage de la cha√Æne RAG...")

    # Cr√©er le retriever (r√©cup√©rateur de documents)
    retriever = vectorstore.as_retriever(
        search_type="similarity",  # Recherche par similarit√©
        search_kwargs={"k": 3}     # R√©cup√©rer les 3 documents les plus similaires
    )

    # Cr√©er la cha√Æne compl√®te
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",        # "stuff" = injecter tous les docs dans le prompt
        retriever=retriever,
        return_source_documents=True,  # Retourner les sources
        chain_type_kwargs={"prompt": prompt}
    )

    print("‚úÖ Cha√Æne RAG pr√™te !")
    return qa_chain


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FONCTION PRINCIPALE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def main():
    """Point d'entr√©e principal du script RAG."""

    # Configuration
    REPERTOIRE_DOCS = "./mes_documents"
    PERSIST_DIR = "./chroma_db"

    # 1. Charger et d√©couper les documents
    chunks = charger_et_decouper_documents(REPERTOIRE_DOCS)

    # 2. Cr√©er la base vectorielle
    vectorstore = creer_base_vectorielle(chunks, PERSIST_DIR)

    # 3. Configurer le LLM
    llm = configurer_llm()

    # 4. Cr√©er le prompt
    prompt = creer_prompt_template()

    # 5. Assembler la cha√Æne RAG
    qa_chain = creer_chaine_rag(vectorstore, llm, prompt)

    # 6. Boucle interactive
    print("\n" + "="*60)
    print("ü§ñ Syst√®me RAG pr√™t ! Posez vos questions.")
    print("   (tapez 'quit' pour quitter)")
    print("="*60 + "\n")

    while True:
        question = input("\n‚ùì Question : ").strip()

        if question.lower() in ['quit', 'exit', 'q']:
            print("üëã Au revoir !")
            break

        if not question:
            continue

        # Ex√©cuter la requ√™te
        print("\nüîç Recherche en cours...")
        resultat = qa_chain.invoke({"query": question})

        # Afficher la r√©ponse
        print("\nüí¨ R√©ponse :")
        print("-" * 60)
        print(resultat['result'])
        print("-" * 60)

        # Afficher les sources
        if resultat.get('source_documents'):
            print("\nüìö Sources utilis√©es :")
            for i, doc in enumerate(resultat['source_documents'], 1):
                source = doc.metadata.get('source', 'Inconnu')
                print(f"   {i}. {source}")


if __name__ == "__main__":
    main()
```

**Commandes d'ex√©cution** :

```bash
# 1. Installer les d√©pendances
pip install langchain chromadb sentence-transformers ollama

# 2. Cr√©er le dossier de documents
mkdir mes_documents
echo "L'intelligence artificielle locale permet..." > mes_documents/doc1.txt
echo "Les mod√®les de langage comme Llama..." > mes_documents/doc2.txt

# 3. S'assurer qu'Ollama est lanc√© avec un mod√®le
ollama run llama3.1:8b

# 4. Lancer le script RAG
python rag_complet.py
```

### 6.2 LoRA et QLoRA (Fine-tuning efficace)

Le [LoRA](#def-lora) (Low-Rank Adaptation) permet d'adapter un mod√®le pr√©-entra√Æn√© avec tr√®s peu de param√®tres suppl√©mentaires.

#### Principe de LoRA

Au lieu de modifier tous les poids du mod√®le (plusieurs milliards de param√®tres), LoRA :

1. **G√®le les poids originaux** du mod√®le (pas de modification)
2. **Ajoute des matrices de faible rang** (quelques millions de param√®tres)
3. **Entra√Æne uniquement ces nouvelles matrices**

```mermaid
graph LR
    A[Mod√®le pr√©-entra√Æn√©<br/>7B param√®tres gel√©s] --> B{LoRA Adapter<br/>4M param√®tres}
    B --> C[Mod√®le adapt√©]

    D[Vos donn√©es<br/>d'entra√Ænement] --> B

    style A fill:#E8F4F8
    style B fill:#FFF4E6
    style C fill:#E8F8F5
    style D fill:#FEF5E7
```

**Avantages** :
- üéØ **99% moins de param√®tres √† entra√Æner** (4M vs 7B)
- ‚ö° **3-5x plus rapide** que le full fine-tuning
- üíæ **√âconomie m√©moire** : 8 GB au lieu de 80 GB
- üîÑ **Modularit√©** : basculer entre plusieurs adapters

#### QLoRA : LoRA + Quantification

[QLoRA](#def-qlora) va encore plus loin en quantifiant le mod√®le de base :

| Caract√©ristique | Full Fine-tuning | LoRA | QLoRA |
|-----------------|------------------|------|-------|
| **Poids entra√Æn√©s** | 100% (7B params) | 0.1% (4M params) | 0.1% (4M params) |
| **M√©moire GPU** | ~80 GB | ~24 GB | **~8 GB** ‚≠ê |
| **VRAM minimale** | 80 GB (A100) | 24 GB (RTX 3090) | **8 GB (RTX 4060)** |
| **Dur√©e (1000 ex)** | 48h | 12h | **6h** |
| **Qualit√©** | 100% | 95-98% | 92-95% |

**Configuration mat√©rielle recommand√©e pour QLoRA** :

- ‚úÖ **GPU 8 GB** : RTX 4060, RTX 3060 12GB
- ‚úÖ **GPU 12-16 GB** : RTX 3080, RTX 4070, RTX 4080
- ‚úÖ **GPU 24 GB** : RTX 3090, RTX 4090 (mod√®les 13B-30B possibles)

#### Code QLoRA avec Unsloth (optimis√©)

```python
"""
Fine-tuning QLoRA optimis√© avec Unsloth pour Llama 3.1.
N√©cessite : pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
"""

from unsloth import FastLanguageModel
import torch
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIGURATION DU MOD√àLE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Param√®tres du mod√®le
max_seq_length = 2048  # Longueur maximale des s√©quences
dtype = None           # Auto-d√©tection (Float16 pour Tesla T4, V100, Bfloat16 pour Ampere+)
load_in_4bit = True    # Quantification 4-bit pour √©conomiser la m√©moire

# Charger le mod√®le avec quantification 4-bit
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Meta-Llama-3.1-8B-bnb-4bit",  # Mod√®le pr√©-quantifi√©
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)

print("‚úÖ Mod√®le Llama 3.1 8B charg√© en 4-bit")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIGURATION LORA
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

model = FastLanguageModel.get_peft_model(
    model,
    r=16,                          # Rang des matrices LoRA (plus √©lev√© = plus de param√®tres)
    target_modules=[               # Modules √† adapter
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj",     # MLP
    ],
    lora_alpha=16,                 # Facteur d'√©chelle LoRA
    lora_dropout=0.0,              # Dropout (0 = plus rapide)
    bias="none",                   # Type de biais
    use_gradient_checkpointing="unsloth",  # √âconomie m√©moire
    random_state=3407,
    use_rslora=False,              # RSLoRA (exp√©rimental)
    loftq_config=None,
)

print(f"‚úÖ LoRA configur√© : r={16}, alpha={16}")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PR√âPARATION DES DONN√âES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Format de prompt pour Llama 3.1 Instruct
alpaca_prompt = """Instruction :
{}

Entr√©e :
{}

R√©ponse :
{}"""

def formater_exemples(examples):
    """
    Formate les exemples au format Llama 3.1 Instruct.

    Args:
        examples: Batch d'exemples du dataset

    Returns:
        Exemples format√©s
    """
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]

    texts = []
    for instruction, input_text, output in zip(instructions, inputs, outputs):
        # Cr√©er le prompt complet
        text = alpaca_prompt.format(instruction, input_text, output)
        texts.append(text)

    return {"text": texts}


# Charger un dataset (exemple avec Alpaca fran√ßais)
# Remplacez par votre propre dataset JSONL
dataset = load_dataset("json", data_files="mes_donnees_entrainement.jsonl", split="train")

# Format attendu du JSONL :
# {"instruction": "R√©sume ce texte", "input": "Lorem ipsum...", "output": "R√©sum√©..."}
# {"instruction": "Traduis en anglais", "input": "Bonjour", "output": "Hello"}

# Formater le dataset
dataset = dataset.map(formater_exemples, batched=True)

print(f"‚úÖ Dataset pr√©par√© : {len(dataset)} exemples")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIGURATION DE L'ENTRA√éNEMENT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",  # Champ contenant le texte
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=False,              # Concat√©ner plusieurs exemples (False = plus simple)
    args=TrainingArguments(
        # Param√®tres de sortie
        output_dir="./resultats_qlora",

        # Param√®tres d'entra√Ænement
        per_device_train_batch_size=2,     # Batch size (augmenter si GPU > 8GB)
        gradient_accumulation_steps=4,     # Accumuler 4 batchs = batch effectif de 8
        warmup_steps=5,                    # Steps de warmup
        num_train_epochs=3,                # Nombre d'√©poques
        learning_rate=2e-4,                # Taux d'apprentissage

        # Optimisation m√©moire
        fp16=not torch.cuda.is_bf16_supported(),  # Mixed precision
        bf16=torch.cuda.is_bf16_supported(),
        optim="adamw_8bit",                # Optimiseur 8-bit

        # Logging et sauvegarde
        logging_steps=10,
        save_strategy="epoch",
        save_total_limit=2,                # Garder seulement 2 checkpoints

        # Autres
        seed=3407,
    ),
)

print("‚úÖ Trainer configur√©")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# LANCER L'ENTRA√éNEMENT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

print("\nüöÄ D√©but de l'entra√Ænement QLoRA...\n")

# Statistiques avant entra√Ænement
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"üíæ M√©moire GPU utilis√©e : {start_gpu_memory} GB / {max_memory} GB ({start_gpu_memory/max_memory*100:.1f}%)")

# Entra√Ænement
trainer_stats = trainer.train()

# Statistiques apr√®s entra√Ænement
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_percent = round(used_memory / max_memory * 100, 3)
print(f"\n‚úÖ Entra√Ænement termin√© !")
print(f"üíæ M√©moire GPU max utilis√©e : {used_memory} GB / {max_memory} GB ({used_memory_percent}%)")
print(f"‚è±Ô∏è  Temps total : {trainer_stats.metrics['train_runtime']:.2f}s")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# SAUVEGARDER LE MOD√àLE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Sauvegarder uniquement l'adapter LoRA (l√©ger, quelques MB)
model.save_pretrained("mon_modele_lora")
tokenizer.save_pretrained("mon_modele_lora")

print("üíæ Adapter LoRA sauvegard√© dans ./mon_modele_lora")

# Optionnel : Sauvegarder le mod√®le fusionn√© complet (lourd, plusieurs GB)
# model.save_pretrained_merged("mon_modele_complet", tokenizer, save_method="merged_16bit")
# print("üíæ Mod√®le complet sauvegard√© dans ./mon_modele_complet")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# INF√âRENCE AVEC LE MOD√àLE FINE-TUN√â
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Activer le mode inf√©rence rapide
FastLanguageModel.for_inference(model)

# Tester le mod√®le
instruction = "Explique ce qu'est le machine learning"
input_text = ""

prompt = alpaca_prompt.format(instruction, input_text, "")
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

# G√©n√©rer
outputs = model.generate(
    **inputs,
    max_new_tokens=256,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)

reponse = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\nüìù Test du mod√®le fine-tun√© :")
print("="*60)
print(reponse)
print("="*60)
```

**Fichier de donn√©es d'entra√Ænement** (`mes_donnees_entrainement.jsonl`) :

```json
{"instruction": "R√©sume ce document", "input": "L'intelligence artificielle est un domaine de l'informatique qui vise √† cr√©er des machines capables de r√©aliser des t√¢ches n√©cessitant normalement l'intelligence humaine.", "output": "L'IA cr√©e des machines intelligentes pour automatiser des t√¢ches cognitives."}
{"instruction": "Traduis en anglais", "input": "Bonjour, comment allez-vous ?", "output": "Hello, how are you?"}
{"instruction": "Corrige les fautes", "input": "Les chein son partis en vacances", "output": "Les chiens sont partis en vacances"}
```

### 6.3 Embeddings et similarit√© vectorielle

Les [embeddings](#def-embeddings) transforment du texte en vecteurs num√©riques capturant le sens s√©mantique.

#### Principe des embeddings

```mermaid
graph LR
    A[" 'intelligence artificielle' "] --> B[Mod√®le d'embedding]
    B --> C["[0.23, -0.45, 0.78, ..., 0.12]<br/>768 dimensions"]

    D[" 'machine learning' "] --> B
    B --> E["[0.25, -0.42, 0.81, ..., 0.15]<br/>768 dimensions"]

    C -.similarit√© cosinus: 0.89.-> E

    style C fill:#E8F8F5
    style E fill:#FEF5E7
```

**Deux phrases similaires = vecteurs proches** ‚Üí Permet la recherche s√©mantique !

#### Comparaison des mod√®les d'embeddings

| Mod√®le | Dimensions | Taille | Multilingue | Performance FR | Vitesse | Recommandation |
|--------|-----------|--------|-------------|----------------|---------|----------------|
| **all-MiniLM-L6-v2** | 384 | 80 MB | ‚ùå Anglais | ‚≠ê‚≠ê Faible | ‚ö°‚ö°‚ö° | Prototypage rapide EN |
| **all-mpnet-base-v2** | 768 | 420 MB | ‚ùå Anglais | ‚≠ê‚≠ê‚≠ê Moyenne | ‚ö°‚ö° | **Bon choix g√©n√©ral EN** ‚úÖ |
| **paraphrase-multilingual-mpnet-base-v2** | 768 | 970 MB | ‚úÖ 50+ langues | ‚≠ê‚≠ê‚≠ê‚≠ê Bonne | ‚ö°‚ö° | **Recommand√© FR** ‚úÖ |
| **multilingual-e5-large** | 1024 | 2.24 GB | ‚úÖ 100 langues | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellente | ‚ö° | **Meilleur FR** ‚≠ê |
| **sentence-t5-xxl** | 768 | 4.8 GB | ‚ùå Anglais | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° | Tr√®s haute qualit√© EN |

**Pour des documents fran√ßais, utilisez** :
1. **multilingual-e5-large** (meilleur qualit√©) si vous avez la RAM
2. **paraphrase-multilingual-mpnet-base-v2** (bon compromis)

#### Code de comparaison d'embeddings

```python
"""
Comparaison de diff√©rents mod√®les d'embeddings pour documents fran√ßais.
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import time


def tester_modele_embeddings(nom_modele: str, phrases: list):
    """
    Teste un mod√®le d'embeddings sur des phrases fran√ßaises.

    Args:
        nom_modele: Nom du mod√®le sur HuggingFace
        phrases: Liste de phrases √† encoder
    """
    print(f"\n{'='*60}")
    print(f"üß™ Test du mod√®le : {nom_modele}")
    print(f"{'='*60}")

    # Charger le mod√®le
    debut = time.time()
    model = SentenceTransformer(nom_modele)
    temps_chargement = time.time() - debut
    print(f"‚è±Ô∏è  Temps de chargement : {temps_chargement:.2f}s")

    # Encoder les phrases
    debut = time.time()
    embeddings = model.encode(phrases, show_progress_bar=False)
    temps_encodage = time.time() - debut

    print(f"üìä Dimensions : {embeddings.shape}")
    print(f"‚è±Ô∏è  Temps d'encodage : {temps_encodage:.3f}s ({len(phrases)} phrases)")
    print(f"‚ö° Vitesse : {len(phrases)/temps_encodage:.1f} phrases/s")

    # Calculer la similarit√© cosinus entre toutes les paires
    print(f"\nüìê Matrice de similarit√© cosinus :")
    similarities = cosine_similarity(embeddings)

    # Afficher la matrice
    print("\n      ", end="")
    for i in range(len(phrases)):
        print(f"P{i+1:2d}    ", end="")
    print()

    for i, row in enumerate(similarities):
        print(f"P{i+1:2d}  ", end="")
        for sim in row:
            print(f"{sim:5.3f}  ", end="")
        print()

    # Trouver les paires les plus similaires (excluant les auto-similarit√©s)
    print(f"\nüîó Paires les plus similaires :")
    paires_similaires = []
    for i in range(len(phrases)):
        for j in range(i+1, len(phrases)):
            paires_similaires.append((i, j, similarities[i][j]))

    paires_similaires.sort(key=lambda x: x[2], reverse=True)

    for i, j, sim in paires_similaires[:3]:
        print(f"   P{i+1} ‚Üî P{j+1} : {sim:.3f}")
        print(f"      '{phrases[i][:50]}...'")
        print(f"      '{phrases[j][:50]}...'")
        print()

    return embeddings, similarities


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PHRASES DE TEST (fran√ßais)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

phrases_test = [
    "L'intelligence artificielle transforme notre quotidien",
    "Les mod√®les de langage comme GPT r√©volutionnent le NLP",
    "Le deep learning n√©cessite beaucoup de donn√©es d'entra√Ænement",
    "J'aime les pizzas et les p√¢tes italiennes",
    "La m√©t√©o est ensoleill√©e aujourd'hui",
    "L'IA et le machine learning sont des technologies cl√©s",
]

print("üìù Phrases de test :")
for i, phrase in enumerate(phrases_test, 1):
    print(f"   P{i} : {phrase}")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TESTER PLUSIEURS MOD√àLES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

modeles_a_tester = [
    "sentence-transformers/all-MiniLM-L6-v2",              # Rapide, anglais
    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",  # Multilingue
    "intfloat/multilingual-e5-large",                      # Meilleur FR
]

resultats = {}

for modele in modeles_a_tester:
    try:
        embeddings, similarities = tester_modele_embeddings(modele, phrases_test)
        resultats[modele] = (embeddings, similarities)
    except Exception as e:
        print(f"‚ùå Erreur avec {modele} : {e}")

print("\n" + "="*60)
print("‚úÖ Tests termin√©s !")
print("="*60)
```

**Sortie attendue** :

```
üìê Matrice de similarit√© cosinus :

       P 1    P 2    P 3    P 4    P 5    P 6
P 1   1.000  0.687  0.521  0.102  0.089  0.821
P 2   0.687  1.000  0.698  0.095  0.112  0.734
P 3   0.521  0.698  1.000  0.087  0.098  0.612
P 4   0.102  0.095  0.087  1.000  0.234  0.098
P 5   0.089  0.112  0.098  0.234  1.000  0.091
P 6   0.821  0.734  0.612  0.098  0.091  1.000

üîó Paires les plus similaires :
   P1 ‚Üî P6 : 0.821
      'L'intelligence artificielle transforme notre quotidien...'
      'L'IA et le machine learning sont des technologies cl√©s...'
```

**Observations** :
- P1 et P6 sont tr√®s similaires (IA/ML)
- P4 et P5 sont dissimilaires des autres (pizza, m√©t√©o)

### 6.4 Choix de l'algorithme : Tableau r√©capitulatif

| Crit√®re | RAG | Fine-tuning (LoRA/QLoRA) | Embeddings seuls |
|---------|-----|--------------------------|------------------|
| **Cas d'usage principal** | QA sur documents √©volutifs | Adaptation style/domaine sp√©cifique | Recherche s√©mantique |
| **Donn√©es n√©cessaires** | Documents (50+) | Exemples entra√Ænement (500+) | Documents √† indexer |
| **Temps de mise en place** | 1-2 jours | 3-7 jours | Quelques heures |
| **GPU requis** | ‚ùå Non (CPU OK) | ‚úÖ Oui (8 GB min) | ‚ùå Non (CPU OK) |
| **M√©moire n√©cessaire** | 4-8 GB RAM | 12-24 GB RAM + 8 GB VRAM | 2-4 GB RAM |
| **Donn√©es √©volutives** | ‚úÖ Ajout facile | ‚ùå R√©-entra√Ænement n√©cessaire | ‚úÖ Ajout facile |
| **Pr√©cision r√©ponses** | ‚≠ê‚≠ê‚≠ê‚≠ê Excellente (sources) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s haute | ‚≠ê‚≠ê‚≠ê Bonne |
| **Co√ªt maintenance** | Faible | Moyen-√©lev√© | Tr√®s faible |
| **Recommandation** | **Commencer ici** ‚úÖ | Si style sp√©cifique n√©cessaire | Pour recherche uniquement |

\newpage

---

## 7. Impl√©mentation RAG Avanc√©e : Application Compl√®te

Cette section pr√©sente une architecture compl√®te d'application [RAG](#def-rag) en production avec interface web, gestion multi-formats et optimisations.

### 7.1 Architecture de l'application

```mermaid
graph TB
    subgraph "Interface Utilisateur"
        A[Interface Web Gradio]
    end

    subgraph "Backend RAG"
        B[API FastAPI]
        C[Gestionnaire de Documents]
        D[Pipeline d'Indexation]
    end

    subgraph "Stockage"
        E[(Base Vectorielle<br/>Chroma)]
        F[(M√©tadonn√©es<br/>SQLite)]
        G[Fichiers Sources]
    end

    subgraph "Mod√®les IA"
        H[Embeddings<br/>HuggingFace]
        I[LLM Ollama<br/>Llama 3.1]
    end

    A -->|Requ√™tes| B
    B -->|Recherche| E
    B -->|M√©tadonn√©es| F
    C -->|Indexation| D
    D -->|Embeddings| H
    D -->|Stockage vecteurs| E
    D -->|M√©tadonn√©es| F
    G -->|Documents| C
    E -->|Contexte| B
    B -->|G√©n√©ration| I
    I -->|R√©ponse| B
    B -->|Affichage| A

    style A fill:#5EA8A7
    style I fill:#FE4447
    style E fill:#F4D03F
    style H fill:#85C1E9
```

### 7.2 Structure du projet

```bash
mon_rag_app/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuration centralis√©e
‚îÇ   ‚îú‚îÄ‚îÄ document_loader.py     # Chargement multi-formats
‚îÇ   ‚îú‚îÄ‚îÄ embeddings_manager.py  # Gestion des embeddings
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py        # Interface Chroma
‚îÇ   ‚îú‚îÄ‚îÄ llm_manager.py         # Gestion Ollama
‚îÇ   ‚îú‚îÄ‚îÄ rag_pipeline.py        # Pipeline RAG complet
‚îÇ   ‚îî‚îÄ‚îÄ api.py                 # API FastAPI
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îî‚îÄ‚îÄ gradio_app.py          # Interface Gradio
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ documents/             # Documents sources
‚îÇ   ‚îú‚îÄ‚îÄ chroma_db/             # Base vectorielle
‚îÇ   ‚îî‚îÄ‚îÄ metadata.db            # Base m√©tadonn√©es
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_rag.py            # Tests unitaires
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ docker-compose.yml         # D√©ploiement Docker
‚îî‚îÄ‚îÄ README.md
```

### 7.3 Code complet : Configuration

**Fichier `src/config.py`** :

```python
"""
Configuration centralis√©e de l'application RAG.
"""

from pathlib import Path
from typing import Literal
import os


class Config:
    """Configuration globale de l'application."""

    # Chemins de base
    BASE_DIR = Path(__file__).parent.parent
    DATA_DIR = BASE_DIR / "data"
    DOCUMENTS_DIR = DATA_DIR / "documents"
    CHROMA_DIR = DATA_DIR / "chroma_db"
    METADATA_DB = DATA_DIR / "metadata.db"

    # Mod√®les
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    EMBEDDING_DEVICE = "cpu"  # ou "cuda" si GPU disponible
    LLM_MODEL = "llama3.1:8b"
    LLM_BASE_URL = "http://localhost:11434"  # URL Ollama

    # Param√®tres RAG
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 50
    TOP_K_RESULTS = 3  # Nombre de documents √† r√©cup√©rer
    SEARCH_TYPE: Literal["similarity", "mmr"] = "similarity"

    # Param√®tres LLM
    TEMPERATURE = 0.2
    MAX_TOKENS = 512
    CONTEXT_WINDOW = 4096

    # API
    API_HOST = "0.0.0.0"
    API_PORT = 8000
    CORS_ORIGINS = ["http://localhost:7860"]  # Pour Gradio

    # Logging
    LOG_LEVEL = "INFO"
    LOG_FILE = BASE_DIR / "rag_app.log"

    # Formats de documents support√©s
    SUPPORTED_FORMATS = {
        ".txt": "text",
        ".pdf": "pdf",
        ".docx": "docx",
        ".md": "markdown",
        ".html": "html",
    }

    @classmethod
    def create_directories(cls):
        """Cr√©e les r√©pertoires n√©cessaires s'ils n'existent pas."""
        cls.DATA_DIR.mkdir(exist_ok=True)
        cls.DOCUMENTS_DIR.mkdir(exist_ok=True)
        cls.CHROMA_DIR.mkdir(exist_ok=True)

    @classmethod
    def validate(cls):
        """Valide la configuration."""
        errors = []

        # V√©rifier Ollama
        try:
            import requests
            response = requests.get(f"{cls.LLM_BASE_URL}/api/tags", timeout=5)
            if response.status_code != 200:
                errors.append(f"Ollama non accessible sur {cls.LLM_BASE_URL}")
        except Exception as e:
            errors.append(f"Erreur connexion Ollama : {e}")

        # V√©rifier le mod√®le Ollama
        try:
            import requests
            response = requests.get(f"{cls.LLM_BASE_URL}/api/tags")
            models = [m["name"] for m in response.json().get("models", [])]
            if cls.LLM_MODEL not in models:
                errors.append(f"Mod√®le {cls.LLM_MODEL} non install√© dans Ollama")
        except:
            pass

        if errors:
            raise ValueError("\n".join(errors))

        print("‚úÖ Configuration valid√©e")


# Cr√©er les r√©pertoires au chargement du module
Config.create_directories()
```

### 7.4 Chargeur de documents multi-formats

**Fichier `src/document_loader.py`** :

```python
"""
Chargeur de documents supportant multiples formats.
"""

from pathlib import Path
from typing import List, Dict, Any
import logging
from datetime import datetime

from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredMarkdownLoader,
    UnstructuredHTMLLoader,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

from .config import Config

logger = logging.getLogger(__name__)


class DocumentLoader:
    """Gestionnaire de chargement de documents multi-formats."""

    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

        # Mappage extension -> classe de loader
        self.loader_mapping = {
            ".txt": TextLoader,
            ".pdf": PyPDFLoader,
            ".docx": Docx2txtLoader,
            ".md": UnstructuredMarkdownLoader,
            ".html": UnstructuredHTMLLoader,
        }

    def load_document(self, file_path: Path) -> List[Document]:
        """
        Charge un document et le d√©coupe en chunks.

        Args:
            file_path: Chemin vers le document

        Returns:
            Liste de chunks avec m√©tadonn√©es
        """
        if not file_path.exists():
            raise FileNotFoundError(f"Fichier introuvable : {file_path}")

        extension = file_path.suffix.lower()
        if extension not in self.loader_mapping:
            raise ValueError(
                f"Format {extension} non support√©. "
                f"Formats accept√©s : {list(self.loader_mapping.keys())}"
            )

        logger.info(f"üìÑ Chargement de {file_path.name}...")

        try:
            # Charger le document
            LoaderClass = self.loader_mapping[extension]
            loader = LoaderClass(str(file_path))
            documents = loader.load()

            # Ajouter des m√©tadonn√©es
            for doc in documents:
                doc.metadata.update({
                    "source": str(file_path),
                    "filename": file_path.name,
                    "file_type": extension,
                    "indexed_at": datetime.now().isoformat(),
                    "file_size": file_path.stat().st_size,
                })

            # D√©couper en chunks
            chunks = self.text_splitter.split_documents(documents)

            logger.info(
                f"‚úÖ {file_path.name} : {len(documents)} docs ‚Üí "
                f"{len(chunks)} chunks"
            )

            return chunks

        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de {file_path.name} : {e}")
            raise

    def load_directory(self, directory: Path) -> List[Document]:
        """
        Charge tous les documents d'un r√©pertoire r√©cursivement.

        Args:
            directory: R√©pertoire contenant les documents

        Returns:
            Liste de tous les chunks
        """
        all_chunks = []
        file_count = 0
        error_count = 0

        logger.info(f"üìÇ Scan du r√©pertoire {directory}...")

        # Parcourir r√©cursivement
        for file_path in directory.rglob("*"):
            if file_path.is_file() and file_path.suffix in self.loader_mapping:
                try:
                    chunks = self.load_document(file_path)
                    all_chunks.extend(chunks)
                    file_count += 1
                except Exception as e:
                    logger.error(f"Erreur avec {file_path.name} : {e}")
                    error_count += 1

        logger.info(
            f"‚úÖ Chargement termin√© : {file_count} fichiers, "
            f"{len(all_chunks)} chunks, {error_count} erreurs"
        )

        return all_chunks

    def get_document_info(self, chunks: List[Document]) -> Dict[str, Any]:
        """
        Extrait des statistiques sur les documents charg√©s.

        Args:
            chunks: Liste de chunks

        Returns:
            Dictionnaire de statistiques
        """
        if not chunks:
            return {}

        # Compter par type de fichier
        file_types = {}
        sources = set()

        for chunk in chunks:
            file_type = chunk.metadata.get("file_type", "unknown")
            file_types[file_type] = file_types.get(file_type, 0) + 1
            sources.add(chunk.metadata.get("source", "unknown"))

        # Calculer la longueur moyenne
        avg_length = sum(len(chunk.page_content) for chunk in chunks) / len(chunks)

        return {
            "total_chunks": len(chunks),
            "total_files": len(sources),
            "file_types": file_types,
            "avg_chunk_length": int(avg_length),
            "sources": list(sources),
        }


# Test du module
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    loader = DocumentLoader()

    # Tester avec un fichier
    test_file = Config.DOCUMENTS_DIR / "test.txt"
    if test_file.exists():
        chunks = loader.load_document(test_file)
        info = loader.get_document_info(chunks)
        print(f"\nüìä Statistiques : {info}")
```

### 7.5 Gestionnaire de base vectorielle

**Fichier `src/vector_store.py`** :

```python
"""
Gestionnaire de la base vectorielle Chroma.
"""

from typing import List, Optional, Dict, Any
import logging
from pathlib import Path

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document

from .config import Config

logger = logging.getLogger(__name__)


class VectorStoreManager:
    """Gestionnaire de la base vectorielle."""

    def __init__(self):
        self.embeddings = self._initialize_embeddings()
        self.vectorstore = None

    def _initialize_embeddings(self) -> HuggingFaceEmbeddings:
        """
        Initialise le mod√®le d'embeddings.

        Returns:
            Mod√®le d'embeddings configur√©
        """
        logger.info(f"üßÆ Chargement du mod√®le d'embeddings {Config.EMBEDDING_MODEL}...")

        embeddings = HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_MODEL,
            model_kwargs={"device": Config.EMBEDDING_DEVICE},
            encode_kwargs={"normalize_embeddings": True}
        )

        logger.info("‚úÖ Mod√®le d'embeddings charg√©")
        return embeddings

    def create_or_load(self, force_recreate: bool = False) -> Chroma:
        """
        Cr√©e ou charge la base vectorielle.

        Args:
            force_recreate: Force la recr√©ation de la base

        Returns:
            Instance Chroma
        """
        if force_recreate and Config.CHROMA_DIR.exists():
            logger.warning("‚ö†Ô∏è  Suppression de la base existante...")
            import shutil
            shutil.rmtree(Config.CHROMA_DIR)

        if Config.CHROMA_DIR.exists() and not force_recreate:
            logger.info(f"üì¶ Chargement de la base depuis {Config.CHROMA_DIR}")
            self.vectorstore = Chroma(
                persist_directory=str(Config.CHROMA_DIR),
                embedding_function=self.embeddings
            )
        else:
            logger.info("üî® Cr√©ation d'une nouvelle base vectorielle")
            self.vectorstore = Chroma(
                persist_directory=str(Config.CHROMA_DIR),
                embedding_function=self.embeddings
            )

        return self.vectorstore

    def add_documents(self, documents: List[Document]) -> None:
        """
        Ajoute des documents √† la base vectorielle.

        Args:
            documents: Liste de documents √† indexer
        """
        if not self.vectorstore:
            self.create_or_load()

        logger.info(f"‚ûï Ajout de {len(documents)} documents...")

        # Ajouter par batch de 100 pour √©viter les probl√®mes de m√©moire
        batch_size = 100
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i+batch_size]
            self.vectorstore.add_documents(batch)
            logger.debug(f"   Batch {i//batch_size + 1} ajout√© ({len(batch)} docs)")

        # Persister
        self.vectorstore.persist()
        logger.info("‚úÖ Documents ajout√©s et base persist√©e")

    def search(
        self,
        query: str,
        k: int = Config.TOP_K_RESULTS,
        filter_dict: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        Recherche dans la base vectorielle.

        Args:
            query: Question de l'utilisateur
            k: Nombre de r√©sultats √† retourner
            filter_dict: Filtre sur les m√©tadonn√©es (ex: {"file_type": ".pdf"})

        Returns:
            Liste de documents pertinents
        """
        if not self.vectorstore:
            self.create_or_load()

        logger.debug(f"üîç Recherche : '{query[:50]}...' (top {k})")

        if Config.SEARCH_TYPE == "similarity":
            results = self.vectorstore.similarity_search(
                query,
                k=k,
                filter=filter_dict
            )
        elif Config.SEARCH_TYPE == "mmr":
            # Maximum Marginal Relevance : diversifie les r√©sultats
            results = self.vectorstore.max_marginal_relevance_search(
                query,
                k=k,
                filter=filter_dict,
                fetch_k=k*3  # R√©cup√©rer 3x plus puis filtrer
            )

        logger.debug(f"‚úÖ {len(results)} documents trouv√©s")
        return results

    def search_with_scores(
        self,
        query: str,
        k: int = Config.TOP_K_RESULTS
    ) -> List[tuple]:
        """
        Recherche avec scores de similarit√©.

        Args:
            query: Question
            k: Nombre de r√©sultats

        Returns:
            Liste de tuples (document, score)
        """
        if not self.vectorstore:
            self.create_or_load()

        results = self.vectorstore.similarity_search_with_score(query, k=k)
        return results

    def get_stats(self) -> Dict[str, Any]:
        """
        R√©cup√®re les statistiques de la base.

        Returns:
            Dictionnaire de statistiques
        """
        if not self.vectorstore:
            self.create_or_load()

        # Chroma n'a pas de m√©thode count() directe, on doit r√©cup√©rer la collection
        collection = self.vectorstore._collection
        count = collection.count()

        return {
            "total_documents": count,
            "embedding_model": Config.EMBEDDING_MODEL,
            "persist_directory": str(Config.CHROMA_DIR),
        }

    def delete_by_source(self, source_path: str) -> None:
        """
        Supprime tous les documents d'une source donn√©e.

        Args:
            source_path: Chemin source √† supprimer
        """
        if not self.vectorstore:
            self.create_or_load()

        logger.info(f"üóëÔ∏è  Suppression des documents de {source_path}")

        # R√©cup√©rer tous les IDs avec ce source
        collection = self.vectorstore._collection
        results = collection.get(where={"source": source_path})

        if results and results["ids"]:
            collection.delete(ids=results["ids"])
            logger.info(f"‚úÖ {len(results['ids'])} documents supprim√©s")
        else:
            logger.info("Aucun document √† supprimer")


# Test du module
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    manager = VectorStoreManager()
    manager.create_or_load()

    stats = manager.get_stats()
    print(f"\nüìä Statistiques : {stats}")
```

### 7.6 Pipeline RAG complet

**Fichier `src/rag_pipeline.py`** :

```python
"""
Pipeline RAG complet avec g√©n√©ration de r√©ponses.
"""

from typing import List, Dict, Any, Optional
import logging

from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document

from .config import Config
from .vector_store import VectorStoreManager

logger = logging.getLogger(__name__)


class RAGPipeline:
    """Pipeline RAG complet."""

    def __init__(self):
        self.vector_store_manager = VectorStoreManager()
        self.vector_store_manager.create_or_load()
        self.llm = self._initialize_llm()
        self.qa_chain = self._create_qa_chain()

    def _initialize_llm(self) -> Ollama:
        """
        Initialise le mod√®le de langage Ollama.

        Returns:
            Instance Ollama configur√©e
        """
        logger.info(f"ü¶ô Configuration d'Ollama ({Config.LLM_MODEL})...")

        llm = Ollama(
            model=Config.LLM_MODEL,
            base_url=Config.LLM_BASE_URL,
            temperature=Config.TEMPERATURE,
            num_ctx=Config.CONTEXT_WINDOW,
            num_predict=Config.MAX_TOKENS,
        )

        logger.info("‚úÖ LLM configur√©")
        return llm

    def _create_prompt_template(self) -> PromptTemplate:
        """
        Cr√©e le template de prompt pour le RAG.

        Returns:
            PromptTemplate configur√©
        """
        template = """Tu es un assistant IA expert qui r√©pond aux questions en te basant sur le contexte fourni.

CONTEXTE :
{context}

QUESTION : {question}

INSTRUCTIONS :
- R√©ponds UNIQUEMENT avec les informations pr√©sentes dans le contexte
- Si la r√©ponse n'est pas dans le contexte, dis clairement "Je n'ai pas trouv√© cette information dans mes documents"
- Sois pr√©cis, structur√© et concis
- Cite les sources si possible (nom de fichier)
- Utilise des listes √† puces ou num√©rot√©es si appropri√©
- R√©ponds en fran√ßais

R√âPONSE :"""

        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

    def _create_qa_chain(self) -> RetrievalQA:
        """
        Cr√©e la cha√Æne QA compl√®te.

        Returns:
            Cha√Æne RetrievalQA
        """
        retriever = self.vector_store_manager.vectorstore.as_retriever(
            search_type=Config.SEARCH_TYPE,
            search_kwargs={"k": Config.TOP_K_RESULTS}
        )

        prompt = self._create_prompt_template()

        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt}
        )

        return qa_chain

    def query(self, question: str) -> Dict[str, Any]:
        """
        Traite une question et g√©n√®re une r√©ponse.

        Args:
            question: Question de l'utilisateur

        Returns:
            Dictionnaire avec r√©ponse et m√©tadonn√©es
        """
        logger.info(f"‚ùì Question : {question[:100]}...")

        try:
            # Ex√©cuter la requ√™te
            result = self.qa_chain.invoke({"query": question})

            # Extraire les sources
            sources = []
            for doc in result.get("source_documents", []):
                sources.append({
                    "filename": doc.metadata.get("filename", "Inconnu"),
                    "file_type": doc.metadata.get("file_type", ""),
                    "excerpt": doc.page_content[:200] + "...",
                })

            response = {
                "question": question,
                "answer": result["result"],
                "sources": sources,
                "num_sources": len(sources),
            }

            logger.info(f"‚úÖ R√©ponse g√©n√©r√©e ({len(response['answer'])} caract√®res)")
            return response

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la requ√™te : {e}")
            return {
                "question": question,
                "answer": f"Erreur : {str(e)}",
                "sources": [],
                "num_sources": 0,
            }

    def query_with_filter(
        self,
        question: str,
        file_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Requ√™te avec filtre sur le type de fichier.

        Args:
            question: Question
            file_type: Extension (.pdf, .txt, etc.)

        Returns:
            R√©ponse avec sources filtr√©es
        """
        # Rechercher avec filtre
        filter_dict = {"file_type": file_type} if file_type else None
        docs = self.vector_store_manager.search(
            question,
            k=Config.TOP_K_RESULTS,
            filter_dict=filter_dict
        )

        if not docs:
            return {
                "question": question,
                "answer": "Aucun document trouv√© avec ce filtre.",
                "sources": [],
                "num_sources": 0,
            }

        # Construire le contexte
        context = "\n\n".join([doc.page_content for doc in docs])

        # G√©n√©rer la r√©ponse
        prompt = self._create_prompt_template()
        formatted_prompt = prompt.format(context=context, question=question)

        answer = self.llm.invoke(formatted_prompt)

        # Formater la r√©ponse
        sources = [{
            "filename": doc.metadata.get("filename", "Inconnu"),
            "file_type": doc.metadata.get("file_type", ""),
            "excerpt": doc.page_content[:200] + "...",
        } for doc in docs]

        return {
            "question": question,
            "answer": answer,
            "sources": sources,
            "num_sources": len(sources),
            "filter": file_type,
        }


# Test du module
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    pipeline = RAGPipeline()

    # Test
    response = pipeline.query("Qu'est-ce que l'intelligence artificielle ?")
    print(f"\nüí¨ R√©ponse : {response['answer']}")
    print(f"\nüìö Sources ({response['num_sources']}) :")
    for i, source in enumerate(response['sources'], 1):
        print(f"   {i}. {source['filename']}")
```

**Commandes de test** :

```bash
# 1. V√©rifier la configuration
python -c "from src.config import Config; Config.validate()"

# 2. Tester le chargement de documents
python src/document_loader.py

# 3. Tester la base vectorielle
python src/vector_store.py

# 4. Tester le pipeline RAG
python src/rag_pipeline.py
```

### 7.7 Points cl√©s d'optimisation

| Optimisation | Impact | Complexit√© | Recommandation |
|--------------|--------|------------|----------------|
| **Chunking adaptatif** | ‚≠ê‚≠ê‚≠ê √âlev√© | Moyenne | Adapter la taille selon le type de doc |
| **Cache des embeddings** | ‚≠ê‚≠ê‚≠ê Tr√®s √©lev√© | Faible | Persister Chroma sur disque ‚úÖ |
| **Batch processing** | ‚≠ê‚≠ê Moyen | Faible | Traiter par lots de 100 docs ‚úÖ |
| **MMR (diversit√©)** | ‚≠ê‚≠ê Moyen | Faible | √âvite les doublons dans r√©sultats |
| **Re-ranking** | ‚≠ê‚≠ê‚≠ê √âlev√© | Moyenne | Mod√®le de re-classement (cross-encoder) |
| **GPU pour embeddings** | ‚≠ê‚≠ê‚≠ê Tr√®s √©lev√© | Faible | 10-50x plus rapide |

\newpage

---

## 8. Fine-tuning Pratique : De la Pr√©paration au D√©ploiement

Cette section d√©taille le processus complet de [fine-tuning](#def-fine-tuning) d'un mod√®le avec [QLoRA](#def-qlora), de la cr√©ation du dataset jusqu'au d√©ploiement.

### 8.1 Workflow de fine-tuning

```mermaid
graph TD
    A[Dataset brut] --> B[Pr√©paration dataset]
    B --> C[Format JSONL]
    C --> D[Validation qualit√©]
    D --> E{Qualit√© OK?}
    E -->|Non| F[Nettoyage/Augmentation]
    F --> B
    E -->|Oui| G[Split train/val/test]
    G --> H[Configuration QLoRA]
    H --> I[Entra√Ænement]
    I --> J[√âvaluation m√©triques]
    J --> K{Performance OK?}
    K -->|Non| L[Ajuster hyperparam√®tres]
    L --> H
    K -->|Oui| M[Sauvegarder adapter LoRA]
    M --> N[Test inf√©rence]
    N --> O[D√©ploiement]

    style A fill:#FEF5E7
    style M fill:#E8F8F5
    style O fill:#5EA8A7
```

### 8.2 Pr√©paration du dataset

#### 8.2.1 Formats de donn√©es

Pour le fine-tuning, plusieurs formats sont possibles :

**Format 1 : Instruction + Entr√©e + Sortie** (Alpaca)

```json
{"instruction": "R√©sume le texte suivant", "input": "Le machine learning est...", "output": "Le ML est une branche de l'IA..."}
{"instruction": "Traduis en anglais", "input": "Bonjour le monde", "output": "Hello world"}
```

**Format 2 : Conversations** (ChatML)

```json
{"messages": [
  {"role": "system", "content": "Tu es un assistant IA expert."},
  {"role": "user", "content": "Qu'est-ce que le RAG ?"},
  {"role": "assistant", "content": "Le RAG (Retrieval-Augmented Generation) combine..."}
]}
```

**Format 3 : Completion** (simple)

```json
{"text": "### Question: Qu'est-ce que Python?\n### R√©ponse: Python est un langage de programmation..."}
```

#### 8.2.2 Script de pr√©paration de dataset

```python
"""
Script de pr√©paration de dataset pour fine-tuning.
Convertit diff√©rents formats en format Alpaca JSONL.
"""

import json
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
from datasets import load_dataset
import re


def nettoyer_texte(texte: str) -> str:
    """
    Nettoie un texte pour le fine-tuning.

    Args:
        texte: Texte brut

    Returns:
        Texte nettoy√©
    """
    # Supprimer espaces multiples
    texte = re.sub(r'\s+', ' ', texte)

    # Supprimer caract√®res de contr√¥le
    texte = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', texte)

    # Trim
    texte = texte.strip()

    return texte


def convertir_csv_en_alpaca(
    csv_path: Path,
    col_instruction: str,
    col_input: str,
    col_output: str
) -> List[Dict[str, str]]:
    """
    Convertit un fichier CSV en format Alpaca.

    Args:
        csv_path: Chemin vers le CSV
        col_instruction: Nom de la colonne instruction
        col_input: Nom de la colonne input
        col_output: Nom de la colonne output

    Returns:
        Liste d'exemples au format Alpaca
    """
    df = pd.read_csv(csv_path)

    exemples = []
    for _, row in df.iterrows():
        exemple = {
            "instruction": nettoyer_texte(str(row[col_instruction])),
            "input": nettoyer_texte(str(row[col_input])) if col_input in row else "",
            "output": nettoyer_texte(str(row[col_output])),
        }
        exemples.append(exemple)

    print(f"‚úÖ {len(exemples)} exemples convertis depuis {csv_path.name}")
    return exemples


def creer_dataset_qa_documents(
    documents_path: Path
) -> List[Dict[str, str]]:
    """
    Cr√©e un dataset Q&A √† partir de documents.
    Exemple : g√©n√®re des paires question-r√©ponse depuis des docs.

    Args:
        documents_path: R√©pertoire contenant les documents

    Returns:
        Dataset au format Alpaca
    """
    exemples = []

    # Exemple simple : extrait titre et premier paragraphe
    for doc_file in documents_path.glob("*.txt"):
        with open(doc_file, 'r', encoding='utf-8') as f:
            contenu = f.read()

        # Strat√©gie simple : d√©couper en sections
        sections = contenu.split('\n\n')
        for i, section in enumerate(sections):
            if len(section) > 50:  # Ignorer les sections trop courtes
                # Cr√©er une question g√©n√©rique
                question = f"Que dit le document sur la section {i+1} ?"

                exemple = {
                    "instruction": question,
                    "input": "",
                    "output": nettoyer_texte(section),
                }
                exemples.append(exemple)

    print(f"‚úÖ {len(exemples)} exemples Q&A g√©n√©r√©s")
    return exemples


def valider_dataset(exemples: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    Valide la qualit√© d'un dataset.

    Args:
        exemples: Liste d'exemples Alpaca

    Returns:
        Rapport de validation
    """
    stats = {
        "total": len(exemples),
        "vides": 0,
        "trop_courts": 0,
        "trop_longs": 0,
        "longueur_moyenne_output": 0,
    }

    longueurs = []

    for ex in exemples:
        output = ex.get("output", "")
        longueur = len(output)
        longueurs.append(longueur)

        if not output or longueur == 0:
            stats["vides"] += 1
        elif longueur < 10:
            stats["trop_courts"] += 1
        elif longueur > 2000:
            stats["trop_longs"] += 1

    if longueurs:
        stats["longueur_moyenne_output"] = sum(longueurs) / len(longueurs)
        stats["longueur_min"] = min(longueurs)
        stats["longueur_max"] = max(longueurs)

    # Calculer le pourcentage de qualit√©
    problemes = stats["vides"] + stats["trop_courts"]
    stats["qualite_pct"] = ((stats["total"] - problemes) / stats["total"] * 100) if stats["total"] > 0 else 0

    return stats


def split_dataset(
    exemples: List[Dict[str, str]],
    train_ratio: float = 0.8,
    val_ratio: float = 0.1,
    test_ratio: float = 0.1,
    shuffle: bool = True
) -> Dict[str, List[Dict[str, str]]]:
    """
    D√©coupe le dataset en train/validation/test.

    Args:
        exemples: Liste d'exemples
        train_ratio: Proportion d'entra√Ænement (0.8 = 80%)
        val_ratio: Proportion de validation
        test_ratio: Proportion de test
        shuffle: M√©langer avant split

    Returns:
        Dict avec 'train', 'val', 'test'
    """
    import random

    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 0.001, "Les ratios doivent sommer √† 1.0"

    if shuffle:
        exemples = exemples.copy()
        random.shuffle(exemples)

    total = len(exemples)
    train_end = int(total * train_ratio)
    val_end = train_end + int(total * val_ratio)

    splits = {
        "train": exemples[:train_end],
        "val": exemples[train_end:val_end],
        "test": exemples[val_end:],
    }

    print(f"üìä Split r√©alis√© :")
    print(f"   Train: {len(splits['train'])} ({len(splits['train'])/total*100:.1f}%)")
    print(f"   Val:   {len(splits['val'])} ({len(splits['val'])/total*100:.1f}%)")
    print(f"   Test:  {len(splits['test'])} ({len(splits['test'])/total*100:.1f}%)")

    return splits


def sauvegarder_jsonl(exemples: List[Dict[str, str]], output_path: Path):
    """
    Sauvegarde le dataset au format JSONL.

    Args:
        exemples: Liste d'exemples
        output_path: Chemin de sortie
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        for exemple in exemples:
            f.write(json.dumps(exemple, ensure_ascii=False) + '\n')

    print(f"üíæ Dataset sauvegard√© : {output_path} ({len(exemples)} exemples)")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# EXEMPLE COMPLET
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

if __name__ == "__main__":
    # 1. Cr√©er un dataset exemple
    exemples = [
        {
            "instruction": "Explique ce qu'est le machine learning",
            "input": "",
            "output": "Le machine learning est une branche de l'intelligence artificielle qui permet aux machines d'apprendre √† partir de donn√©es sans √™tre explicitement programm√©es.",
        },
        {
            "instruction": "Traduis en anglais",
            "input": "Bonjour le monde",
            "output": "Hello world",
        },
        {
            "instruction": "R√©sume ce texte",
            "input": "Le RAG (Retrieval-Augmented Generation) combine la recherche de documents avec la g√©n√©ration de texte. Cette approche permet de r√©pondre √† des questions en se basant sur une base de connaissances sp√©cifique.",
            "output": "Le RAG combine recherche documentaire et g√©n√©ration de texte pour r√©pondre aux questions via une base de connaissances.",
        },
    ]

    # 2. Valider
    stats = valider_dataset(exemples)
    print(f"\nüìä Statistiques de validation :")
    print(f"   Total: {stats['total']}")
    print(f"   Qualit√©: {stats['qualite_pct']:.1f}%")
    print(f"   Longueur moyenne output: {stats['longueur_moyenne_output']:.0f} caract√®res")

    # 3. Split
    splits = split_dataset(exemples, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)

    # 4. Sauvegarder
    output_dir = Path("./dataset_finetuning")
    output_dir.mkdir(exist_ok=True)

    sauvegarder_jsonl(splits['train'], output_dir / "train.jsonl")
    sauvegarder_jsonl(splits['val'], output_dir / "val.jsonl")
    sauvegarder_jsonl(splits['test'], output_dir / "test.jsonl")

    print("\n‚úÖ Dataset pr√™t pour le fine-tuning !")
```

### 8.3 Entra√Ænement avec monitoring

**Script d'entra√Ænement avanc√© avec logging** :

```python
"""
Script de fine-tuning QLoRA avec monitoring et logging d√©taill√©.
"""

from unsloth import FastLanguageModel
import torch
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments, TrainerCallback
import wandb  # Weights & Biases pour le monitoring (optionnel)
from datetime import datetime
import json


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIGURATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class Config:
    """Configuration du fine-tuning."""

    # Mod√®le
    MODEL_NAME = "unsloth/Meta-Llama-3.1-8B-bnb-4bit"
    MAX_SEQ_LENGTH = 2048

    # LoRA
    LORA_R = 16
    LORA_ALPHA = 16
    LORA_DROPOUT = 0.0

    # Entra√Ænement
    BATCH_SIZE = 2
    GRADIENT_ACCUMULATION_STEPS = 4  # Batch effectif = 8
    LEARNING_RATE = 2e-4
    NUM_EPOCHS = 3
    WARMUP_STEPS = 10
    SAVE_STEPS = 100
    LOGGING_STEPS = 10

    # Datasets
    TRAIN_FILE = "dataset_finetuning/train.jsonl"
    VAL_FILE = "dataset_finetuning/val.jsonl"

    # Output
    OUTPUT_DIR = f"./models/llama_finetuned_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    # Monitoring (optionnel)
    USE_WANDB = False
    WANDB_PROJECT = "llama-finetuning"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CALLBACK PERSONNALIS√â POUR LOGGING
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class DetailedLoggingCallback(TrainerCallback):
    """Callback pour logger les d√©tails de l'entra√Ænement."""

    def __init__(self):
        self.start_time = None
        self.logs = []

    def on_train_begin(self, args, state, control, **kwargs):
        """D√©but de l'entra√Ænement."""
        self.start_time = datetime.now()
        print(f"\nüöÄ D√©but de l'entra√Ænement : {self.start_time.strftime('%H:%M:%S')}")
        print(f"üìä Epochs: {args.num_train_epochs}")
        print(f"üì¶ Batch size: {args.per_device_train_batch_size}")
        print(f"üìà Steps par epoch: {state.max_steps // args.num_train_epochs}\n")

    def on_log(self, args, state, control, logs=None, **kwargs):
        """√Ä chaque log."""
        if logs:
            self.logs.append(logs)

            # Afficher la progression
            if "loss" in logs:
                step = state.global_step
                loss = logs["loss"]
                lr = logs.get("learning_rate", "N/A")

                print(f"   Step {step:4d} | Loss: {loss:.4f} | LR: {lr}")

    def on_epoch_end(self, args, state, control, **kwargs):
        """Fin d'une epoch."""
        epoch = int(state.epoch)
        print(f"\n‚úÖ Epoch {epoch} termin√©e\n")

    def on_train_end(self, args, state, control, **kwargs):
        """Fin de l'entra√Ænement."""
        end_time = datetime.now()
        duration = end_time - self.start_time

        print(f"\nüéâ Entra√Ænement termin√© !")
        print(f"‚è±Ô∏è  Dur√©e totale: {duration}")
        print(f"üìä Steps totaux: {state.global_step}")

        # Sauvegarder les logs
        log_file = f"{args.output_dir}/training_logs.json"
        with open(log_file, 'w') as f:
            json.dump(self.logs, f, indent=2)
        print(f"üìù Logs sauvegard√©s : {log_file}")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FONCTION PRINCIPALE D'ENTRA√éNEMENT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def train():
    """Lance le fine-tuning."""

    # 1. Charger le mod√®le
    print("üì¶ Chargement du mod√®le...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=Config.MODEL_NAME,
        max_seq_length=Config.MAX_SEQ_LENGTH,
        dtype=None,
        load_in_4bit=True,
    )

    # 2. Configurer LoRA
    print("üîß Configuration LoRA...")
    model = FastLanguageModel.get_peft_model(
        model,
        r=Config.LORA_R,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                        "gate_proj", "up_proj", "down_proj"],
        lora_alpha=Config.LORA_ALPHA,
        lora_dropout=Config.LORA_DROPOUT,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=3407,
    )

    # Afficher le nombre de param√®tres entra√Ænables
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"üéØ Param√®tres entra√Ænables: {trainable_params:,} / {total_params:,} "
          f"({100 * trainable_params / total_params:.2f}%)")

    # 3. Format de prompt
    alpaca_prompt = """Instruction :
{}

Entr√©e :
{}

R√©ponse :
{}"""

    def formater_exemples(examples):
        """Formate les exemples."""
        instructions = examples["instruction"]
        inputs = examples["input"]
        outputs = examples["output"]

        texts = []
        for instruction, input_text, output in zip(instructions, inputs, outputs):
            text = alpaca_prompt.format(instruction, input_text, output)
            texts.append(text)

        return {"text": texts}

    # 4. Charger les datasets
    print(f"üìÇ Chargement du dataset d'entra√Ænement...")
    train_dataset = load_dataset("json", data_files=Config.TRAIN_FILE, split="train")
    train_dataset = train_dataset.map(formater_exemples, batched=True)

    print(f"üìÇ Chargement du dataset de validation...")
    val_dataset = load_dataset("json", data_files=Config.VAL_FILE, split="train")
    val_dataset = val_dataset.map(formater_exemples, batched=True)

    print(f"‚úÖ Train: {len(train_dataset)} exemples")
    print(f"‚úÖ Val: {len(val_dataset)} exemples")

    # 5. Initialiser Weights & Biases (optionnel)
    if Config.USE_WANDB:
        wandb.init(project=Config.WANDB_PROJECT, config=vars(Config))

    # 6. Configuration du Trainer
    training_args = TrainingArguments(
        output_dir=Config.OUTPUT_DIR,
        per_device_train_batch_size=Config.BATCH_SIZE,
        gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,
        warmup_steps=Config.WARMUP_STEPS,
        num_train_epochs=Config.NUM_EPOCHS,
        learning_rate=Config.LEARNING_RATE,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=Config.LOGGING_STEPS,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        save_strategy="steps",
        save_steps=Config.SAVE_STEPS,
        save_total_limit=3,
        evaluation_strategy="steps",
        eval_steps=Config.SAVE_STEPS,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        report_to="wandb" if Config.USE_WANDB else "none",
    )

    # 7. Cr√©er le Trainer
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        dataset_text_field="text",
        max_seq_length=Config.MAX_SEQ_LENGTH,
        dataset_num_proc=2,
        packing=False,
        args=training_args,
        callbacks=[DetailedLoggingCallback()],
    )

    # 8. Statistiques GPU avant entra√Ænement
    if torch.cuda.is_available():
        gpu_stats = torch.cuda.get_device_properties(0)
        start_memory = torch.cuda.max_memory_reserved() / 1024**3
        max_memory = gpu_stats.total_memory / 1024**3
        print(f"\nüíæ M√©moire GPU initiale: {start_memory:.2f} GB / {max_memory:.2f} GB")

    # 9. ENTRA√éNEMENT
    print("\n" + "="*60)
    print("üöÄ D√âBUT DE L'ENTRA√éNEMENT")
    print("="*60 + "\n")

    trainer_stats = trainer.train()

    # 10. Statistiques finales
    if torch.cuda.is_available():
        used_memory = torch.cuda.max_memory_reserved() / 1024**3
        print(f"\nüíæ M√©moire GPU maximale utilis√©e: {used_memory:.2f} GB / {max_memory:.2f} GB "
              f"({used_memory/max_memory*100:.1f}%)")

    print(f"\n‚è±Ô∏è  Temps d'entra√Ænement: {trainer_stats.metrics['train_runtime']:.2f}s")
    print(f"üìâ Loss finale: {trainer_stats.metrics['train_loss']:.4f}")

    # 11. Sauvegarder le mod√®le final
    print(f"\nüíæ Sauvegarde du mod√®le dans {Config.OUTPUT_DIR}...")
    model.save_pretrained(Config.OUTPUT_DIR)
    tokenizer.save_pretrained(Config.OUTPUT_DIR)

    print("\n‚úÖ FINE-TUNING TERMIN√â !")
    print(f"üìÅ Mod√®le sauvegard√© : {Config.OUTPUT_DIR}")


if __name__ == "__main__":
    train()
```

### 8.4 √âvaluation du mod√®le fine-tun√©

**Script d'√©valuation** :

```python
"""
√âvaluation d'un mod√®le fine-tun√©.
"""

from unsloth import FastLanguageModel
import torch
from datasets import load_dataset
import json
from typing import List, Dict
from pathlib import Path


def charger_modele_finetuned(model_path: str):
    """
    Charge un mod√®le fine-tun√©.

    Args:
        model_path: Chemin vers le mod√®le

    Returns:
        Mod√®le et tokenizer
    """
    print(f"üì¶ Chargement du mod√®le depuis {model_path}...")

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_path,
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )

    # Activer le mode inf√©rence rapide
    FastLanguageModel.for_inference(model)

    print("‚úÖ Mod√®le charg√©")
    return model, tokenizer


def generer_reponse(
    model,
    tokenizer,
    instruction: str,
    input_text: str = "",
    max_tokens: int = 256
) -> str:
    """
    G√©n√®re une r√©ponse avec le mod√®le.

    Args:
        model: Mod√®le
        tokenizer: Tokenizer
        instruction: Instruction
        input_text: Entr√©e optionnelle
        max_tokens: Nombre max de tokens

    Returns:
        R√©ponse g√©n√©r√©e
    """
    alpaca_prompt = """Instruction :
{}

Entr√©e :
{}

R√©ponse :
{}"""

    prompt = alpaca_prompt.format(instruction, input_text, "")
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens=max_tokens,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
    )

    reponse_complete = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extraire seulement la partie r√©ponse
    if "R√©ponse :" in reponse_complete:
        reponse = reponse_complete.split("R√©ponse :")[1].strip()
    else:
        reponse = reponse_complete

    return reponse


def evaluer_sur_dataset(
    model,
    tokenizer,
    test_file: str,
    output_file: str = "evaluation_results.json"
):
    """
    √âvalue le mod√®le sur un dataset de test.

    Args:
        model: Mod√®le
        tokenizer: Tokenizer
        test_file: Fichier JSONL de test
        output_file: Fichier de sortie des r√©sultats
    """
    print(f"üìÇ Chargement du dataset de test : {test_file}")
    dataset = load_dataset("json", data_files=test_file, split="train")

    resultats = []

    print(f"\nüß™ √âvaluation sur {len(dataset)} exemples...\n")

    for i, exemple in enumerate(dataset):
        instruction = exemple["instruction"]
        input_text = exemple.get("input", "")
        expected = exemple["output"]

        # G√©n√©rer la r√©ponse
        predicted = generer_reponse(model, tokenizer, instruction, input_text)

        # Stocker le r√©sultat
        resultat = {
            "index": i,
            "instruction": instruction,
            "input": input_text,
            "expected": expected,
            "predicted": predicted,
        }
        resultats.append(resultat)

        # Afficher
        print(f"Exemple {i+1}/{len(dataset)}")
        print(f"   Instruction: {instruction[:60]}...")
        print(f"   Attendu: {expected[:60]}...")
        print(f"   Pr√©dit:  {predicted[:60]}...")
        print()

    # Sauvegarder les r√©sultats
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(resultats, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ R√©sultats sauvegard√©s : {output_file}")

    return resultats


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# EXEMPLE D'UTILISATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

if __name__ == "__main__":
    # 1. Charger le mod√®le fine-tun√©
    MODEL_PATH = "./models/llama_finetuned_20250125_143000"
    model, tokenizer = charger_modele_finetuned(MODEL_PATH)

    # 2. Test interactif
    print("\n" + "="*60)
    print("ü§ñ Mode test interactif")
    print("="*60 + "\n")

    tests = [
        ("Explique ce qu'est le RAG", ""),
        ("Traduis en anglais", "Bonjour le monde"),
        ("R√©sume ce texte", "L'intelligence artificielle locale permet..."),
    ]

    for instruction, input_text in tests:
        reponse = generer_reponse(model, tokenizer, instruction, input_text)
        print(f"‚ùì {instruction}")
        if input_text:
            print(f"üìù Entr√©e: {input_text}")
        print(f"üí¨ R√©ponse: {reponse}\n")

    # 3. √âvaluation sur dataset de test
    # evaluer_sur_dataset(model, tokenizer, "dataset_finetuning/test.jsonl")
```

**Commandes d'ex√©cution** :

```bash
# 1. Pr√©parer le dataset
python prepare_dataset.py

# 2. Lancer l'entra√Ænement
python train_qlora.py

# 3. √âvaluer le mod√®le
python evaluate_model.py

# 4. D√©ployer avec Ollama
ollama create mon_modele_custom -f Modelfile
ollama run mon_modele_custom
```

### 8.5 D√©ploiement du mod√®le fine-tun√©

**Fichier `Modelfile` pour Ollama** :

```dockerfile
# Modelfile pour d√©ployer un mod√®le fine-tun√© dans Ollama

FROM ./models/llama_finetuned_20250125_143000

# Param√®tres syst√®me
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_ctx 4096

# Prompt syst√®me
SYSTEM """Tu es un assistant IA expert qui a √©t√© sp√©cialement entra√Æn√© pour r√©pondre aux questions sur [VOTRE DOMAINE].

Tes caract√©ristiques :
- Tu es pr√©cis et factuel
- Tu cites tes sources quand possible
- Tu admets quand tu ne sais pas
- Tu r√©ponds en fran√ßais
"""

# Template de conversation
TEMPLATE """{{ if .System }}### Syst√®me:
{{ .System }}

{{ end }}### Instruction:
{{ .Prompt }}

### R√©ponse:
"""
```

**Commandes de d√©ploiement** :

```bash
# Cr√©er le mod√®le Ollama
ollama create mon_ia_custom -f Modelfile

# Lister les mod√®les
ollama list

# Tester le mod√®le
ollama run mon_ia_custom

# D√©marrer en serveur API
ollama serve
```

### 8.6 M√©triques et benchmarks

| M√©trique | Description | Commande/Outil |
|----------|-------------|----------------|
| **Perplexity** | Mesure de confusion du mod√®le | `trainer.evaluate()` |
| **BLEU Score** | Similarit√© avec r√©f√©rence (traduction) | `nltk.translate.bleu_score` |
| **ROUGE Score** | Qualit√© de r√©sum√©s | `rouge_score` |
| **Exact Match** | Correspondance exacte | Custom |
| **Loss** | Perte d'entra√Ænement/validation | Logs Trainer |

\newpage

---

## 9. D√©ploiement API et Production

Cette section couvre le d√©ploiement d'une [API](#def-api) de production pour votre IA locale.

### 9.1 API FastAPI compl√®te

**Fichier `api.py`** :

```python
"""
API FastAPI pour servir un syst√®me RAG ou un mod√®le fine-tun√©.
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
from datetime import datetime
import logging

# Importer vos modules RAG
from src.rag_pipeline import RAGPipeline
from src.config import Config

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialiser l'application FastAPI
app = FastAPI(
    title="API IA Locale",
    description="API pour syst√®me RAG et mod√®les fine-tun√©s",
    version="1.0.0"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=Config.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialiser le pipeline RAG au d√©marrage
pipeline = None

@app.on_event("startup")
async def startup_event():
    """Initialisation au d√©marrage de l'API."""
    global pipeline
    logger.info("üöÄ D√©marrage de l'API...")

    try:
        pipeline = RAGPipeline()
        logger.info("‚úÖ Pipeline RAG initialis√©")
    except Exception as e:
        logger.error(f"‚ùå Erreur initialisation pipeline : {e}")
        raise


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MOD√àLES DE DONN√âES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class QueryRequest(BaseModel):
    """Mod√®le de requ√™te."""
    question: str
    max_tokens: Optional[int] = 512
    temperature: Optional[float] = 0.7
    file_type_filter: Optional[str] = None

    class Config:
        schema_extra = {
            "example": {
                "question": "Qu'est-ce que le RAG ?",
                "max_tokens": 512,
                "temperature": 0.7
            }
        }


class Source(BaseModel):
    """Mod√®le de source."""
    filename: str
    file_type: str
    excerpt: str


class QueryResponse(BaseModel):
    """Mod√®le de r√©ponse."""
    question: str
    answer: str
    sources: List[Source]
    num_sources: int
    timestamp: str


class HealthResponse(BaseModel):
    """Mod√®le de sant√© de l'API."""
    status: str
    timestamp: str
    pipeline_ready: bool
    version: str


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ENDPOINTS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@app.get("/", response_model=Dict[str, str])
async def root():
    """Endpoint racine."""
    return {
        "message": "API IA Locale",
        "version": "1.0.0",
        "docs": "/docs"
    }


@app.get("/health", response_model=HealthResponse)
async def health():
    """V√©rifie la sant√© de l'API."""
    return HealthResponse(
        status="healthy" if pipeline else "unhealthy",
        timestamp=datetime.now().isoformat(),
        pipeline_ready=pipeline is not None,
        version="1.0.0"
    )


@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """
    Traite une question et retourne une r√©ponse.

    Args:
        request: Requ√™te contenant la question

    Returns:
        R√©ponse avec sources
    """
    if not pipeline:
        raise HTTPException(status_code=503, detail="Pipeline non initialis√©")

    logger.info(f"üì• Question re√ßue : {request.question[:50]}...")

    try:
        # Ex√©cuter la requ√™te
        if request.file_type_filter:
            result = pipeline.query_with_filter(
                request.question,
                file_type=request.file_type_filter
            )
        else:
            result = pipeline.query(request.question)

        # Convertir en mod√®le Pydantic
        response = QueryResponse(
            question=result["question"],
            answer=result["answer"],
            sources=[Source(**s) for s in result["sources"]],
            num_sources=result["num_sources"],
            timestamp=datetime.now().isoformat()
        )

        logger.info(f"‚úÖ R√©ponse g√©n√©r√©e ({len(response.answer)} caract√®res)")
        return response

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du traitement : {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/stats")
async def get_stats():
    """R√©cup√®re les statistiques de la base vectorielle."""
    if not pipeline:
        raise HTTPException(status_code=503, detail="Pipeline non initialis√©")

    try:
        stats = pipeline.vector_store_manager.get_stats()
        return stats
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration stats : {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# LANCEMENT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

if __name__ == "__main__":
    uvicorn.run(
        "api:app",
        host=Config.API_HOST,
        port=Config.API_PORT,
        reload=True,
        log_level="info"
    )
```

**Fichier `docker-compose.yml`** :

```yaml
version: '3.8'

services:
  # Service Ollama
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Service API RAG
  api:
    build: .
    container_name: ia_locale_api
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./src:/app/src
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

  # Interface Gradio (optionnel)
  gradio:
    build:
      context: .
      dockerfile: Dockerfile.gradio
    container_name: ia_locale_ui
    ports:
      - "7860:7860"
    depends_on:
      - api
    restart: unless-stopped

volumes:
  ollama_data:
```

**Fichier `Dockerfile`** :

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Installation des d√©pendances syst√®me
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copier les requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copier le code
COPY src/ ./src/
COPY api.py .

# Exposer le port
EXPOSE 8000

# Commande de d√©marrage
CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Commandes de d√©ploiement** :

```bash
# 1. Construire et lancer tous les services
docker-compose up -d

# 2. V√©rifier les logs
docker-compose logs -f

# 3. Tester l'API
curl http://localhost:8000/health

# 4. Arr√™ter les services
docker-compose down
```

\newpage

---

## 10. √âvaluation et Optimisation

### 10.1 M√©triques d'√©valuation

| M√©trique | Utilisation | Bon score | Commande |
|----------|-------------|-----------|----------|
| **Latence** | Temps de r√©ponse | < 2s | `time curl ...` |
| **Throughput** | Requ√™tes/sec | > 10 req/s | `apache bench` |
| **Pr√©cision** | Qualit√© RAG | > 85% | √âvaluation manuelle |
| **VRAM** | Utilisation GPU | < 80% | `nvidia-smi` |
| **CPU** | Utilisation CPU | < 70% | `htop` |

### 10.2 Optimisations cl√©s

**1. Cache de r√©ponses** :

```python
from functools import lru_cache
import hashlib

class CachedRAGPipeline(RAGPipeline):
    """Pipeline RAG avec cache."""

    @lru_cache(maxsize=100)
    def query_cached(self, question: str):
        """Requ√™te avec cache."""
        return self.query(question)
```

**2. Batch processing** :

```python
def query_batch(questions: List[str]) -> List[Dict]:
    """Traite plusieurs questions en lot."""
    # Recherche vectorielle en batch
    all_results = vectorstore.similarity_search_batch(questions, k=3)

    # G√©n√©ration en batch
    contexts = [format_context(res) for res in all_results]
    answers = llm.generate(contexts)

    return answers
```

**3. Quantification du mod√®le** :

```bash
# Quantifier en 4-bit avec llama.cpp
python convert.py models/llama --outtype q4_0 --outfile model_q4.gguf

# Utiliser avec Ollama
ollama create mon_modele_q4 -f Modelfile.q4
```

### 10.3 Monitoring avec Prometheus

**Fichier `prometheus.yml`** :

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'fastapi'
    static_configs:
      - targets: ['localhost:8000']
```

\newpage

---

## 11. S√©curit√©, Anonymisation et RGPD

### 11.1 S√©curit√© de l'API

```python
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi import Depends, HTTPException
import os

security = HTTPBearer()

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """V√©rifie le token d'authentification."""
    token = credentials.credentials
    expected_token = os.getenv("API_TOKEN")

    if token != expected_token:
        raise HTTPException(status_code=401, detail="Token invalide")

    return token

# Prot√©ger les endpoints
@app.post("/query", dependencies=[Depends(verify_token)])
async def query(request: QueryRequest):
    ...
```

### 11.2 Anonymisation des donn√©es

```python
import re
import hashlib

def anonymiser_texte(texte: str) -> str:
    """
    Anonymise les donn√©es personnelles dans un texte.

    Args:
        texte: Texte √† anonymiser

    Returns:
        Texte anonymis√©
    """
    # Emails
    texte = re.sub(
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        '[EMAIL]',
        texte
    )

    # T√©l√©phones fran√ßais
    texte = re.sub(
        r'\b0[1-9](?:[\s.-]?\d{2}){4}\b',
        '[TELEPHONE]',
        texte
    )

    # Num√©ros de s√©curit√© sociale
    texte = re.sub(
        r'\b[12]\s?\d{2}\s?\d{2}\s?\d{2}\s?\d{3}\s?\d{3}\s?\d{2}\b',
        '[NUM_SECU]',
        texte
    )

    # Noms propres (heuristique simple)
    # Note : utiliser un mod√®le NER pour plus de pr√©cision
    texte = re.sub(
        r'\b[A-Z][a-z]+ [A-Z][a-z]+\b',
        '[NOM]',
        texte
    )

    return texte


def hasher_identifiant(identifiant: str) -> str:
    """
    Hash un identifiant de mani√®re irr√©versible.

    Args:
        identifiant: Identifiant √† hasher

    Returns:
        Hash SHA-256
    """
    return hashlib.sha256(identifiant.encode()).hexdigest()[:16]
```

### 11.3 Conformit√© RGPD

**Checklist RGPD pour IA locale** :

- ‚úÖ **Minimisation des donn√©es** : Ne collecter que les donn√©es n√©cessaires
- ‚úÖ **Traitement local** : Toutes les donn√©es restent sur vos serveurs
- ‚úÖ **Droit √† l'oubli** : Fonction de suppression des documents index√©s
- ‚úÖ **Transparence** : Informer les utilisateurs du traitement
- ‚úÖ **S√©curit√©** : Chiffrement, authentification, logs

**Fonction de suppression (droit √† l'oubli)** :

```python
def supprimer_donnees_utilisateur(user_id: str):
    """
    Supprime toutes les donn√©es d'un utilisateur.

    Args:
        user_id: Identifiant utilisateur
    """
    # 1. Supprimer de la base vectorielle
    vectorstore_manager.delete_by_metadata({"user_id": user_id})

    # 2. Supprimer les fichiers sources
    user_docs_path = f"data/documents/user_{user_id}"
    if os.path.exists(user_docs_path):
        shutil.rmtree(user_docs_path)

    # 3. Supprimer les logs
    db.execute("DELETE FROM logs WHERE user_id = ?", (user_id,))

    logger.info(f"‚úÖ Donn√©es de l'utilisateur {user_id} supprim√©es")
```

### 11.4 Chiffrement des donn√©es

```python
from cryptography.fernet import Fernet

class ChiffrementManager:
    """Gestionnaire de chiffrement pour les donn√©es sensibles."""

    def __init__(self):
        # Charger ou g√©n√©rer une cl√©
        key_file = "encryption.key"
        if os.path.exists(key_file):
            with open(key_file, 'rb') as f:
                self.key = f.read()
        else:
            self.key = Fernet.generate_key()
            with open(key_file, 'wb') as f:
                f.write(self.key)

        self.cipher = Fernet(self.key)

    def chiffrer(self, data: str) -> bytes:
        """Chiffre une cha√Æne."""
        return self.cipher.encrypt(data.encode())

    def dechiffrer(self, encrypted_data: bytes) -> str:
        """D√©chiffre une cha√Æne."""
        return self.cipher.decrypt(encrypted_data).decode()
```

\newpage

---

## 12. Conclusion et Ressources

### 12.1 R√©capitulatif

Ce guide a couvert l'int√©gralit√© du processus de cr√©ation d'une IA locale :

1. ‚úÖ **Installation** (Section 3) : Environnement Python, Ollama, d√©pendances
2. ‚úÖ **Choix de l'approche** (Section 4) : RAG vs Fine-tuning
3. ‚úÖ **Pr√©paration des donn√©es** (Section 5) : Collecte, extraction, nettoyage
4. ‚úÖ **Algorithmes** (Section 6) : RAG, LoRA/QLoRA, Embeddings
5. ‚úÖ **Impl√©mentation RAG** (Section 7) : Application compl√®te multi-formats
6. ‚úÖ **Fine-tuning** (Section 8) : Pr√©paration dataset, entra√Ænement, d√©ploiement
7. ‚úÖ **D√©ploiement** (Section 9) : API FastAPI, Docker, production
8. ‚úÖ **Optimisation** (Section 10) : M√©triques, cache, monitoring
9. ‚úÖ **S√©curit√©** (Section 11) : Anonymisation, RGPD, chiffrement

### 12.2 Prochaines √©tapes recommand√©es

**Pour d√©buter** :
1. Commencer par un projet RAG simple avec Ollama + Chroma
2. Tester sur 10-20 documents de votre domaine
3. √âvaluer la qualit√© des r√©ponses
4. It√©rer sur le prompt et les param√®tres

**Pour aller plus loin** :
1. Impl√©menter un fine-tuning QLoRA sur un dataset sp√©cifique
2. D√©ployer une API en production avec Docker
3. Ajouter une interface Gradio pour les utilisateurs
4. Mettre en place un monitoring avec Prometheus

### 12.3 Ressources et liens utiles

**Documentation officielle** :
- Ollama : https://ollama.com/docs
- LangChain : https://python.langchain.com/docs
- Hugging Face : https://huggingface.co/docs
- Unsloth : https://github.com/unslothai/unsloth

**Mod√®les recommand√©s** :
- Llama 3.1 8B : Meilleur compromis qualit√©/performance
- Mistral 7B : Excellent pour le fran√ßais
- Phi-3 : Tr√®s efficace pour les petits GPUs (< 8 GB)

**Communaut√©s** :
- Reddit : r/LocalLLaMA
- Discord : LangChain, Ollama
- GitHub Discussions : huggingface/transformers

**Commandes de conversion finale** :

```bash
# G√©n√©rer le guide DOCX avec table des mati√®res
pandoc guide_technique_detaille.md \
  -o guide_technique_detaille.docx \
  --toc \
  --toc-depth=3 \
  -V geometry:margin=2.5cm \
  -V fontsize=11pt \
  -V documentclass=report

# V√©rifier le fichier g√©n√©r√©
ls -lh guide_technique_detaille.docx

# G√©n√©rer un PDF (optionnel)
pandoc guide_technique_detaille.md \
  -o guide_technique_detaille.pdf \
  --toc \
  --pdf-engine=xelatex
```

---

**üìù FIN DU GUIDE**

**Version** : 1.0
**Derni√®re mise √† jour** : Octobre 2025
**Auteur** : G√©n√©r√© avec Claude Code

---
