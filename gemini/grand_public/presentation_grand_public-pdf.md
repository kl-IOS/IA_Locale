# ü§ñ L‚ÄôIA Locale, une r√©volution √† port√©e de main
üí¨ *¬´ D√©couvrez comment rendre l‚ÄôIA accessible √† tous, sans cloud ni d√©pendance. ¬ª*

### Pourquoi s‚Äôy int√©resser ?
- üí° **Autonome :** fonctionne sans cloud ni abonnement.
- üîí **S√ªre :** vos donn√©es restent sur votre ordinateur.
- ‚öôÔ∏è **Personnalisable :** adaptez l‚ÄôIA √† vos usages.
- ‚ö° **Efficace :** des performances proches des solutions cloud.

> üéØ *Objectif :* vous montrer **comment mettre en place votre propre IA locale** en quelques √©tapes simples.

---

# Cr√©ez votre IA Locale üöÄ
Un guide pratique pour ma√Ætriser l‚Äôintelligence artificielle sur votre machine.

---

## Qu‚Äôest‚Äëce qu‚Äôune IA locale ?
- **Confidentialit√© :** Vos donn√©es restent sur votre machine, jamais envoy√©es √† des serveurs externes.
- **Autonomie :** Fonctionne sans connexion internet, utile hors‚Äëligne.
- **Ma√Ætrise :** Contr√¥le total sur l‚Äôoutil, ses mod√®les et ses mises √† jour.

---

## Mat√©riel {.subslide}
üíª **Ce qu‚Äôil faut id√©alement :**
- **Processeur (CPU) :** i7/Ryzen 7 recommand√© (i5/Ryzen 5 minimum).
- **M√©moire vive (RAM) :** 32 Go recommand√© (16 Go minimum).
- **Carte graphique (GPU) :** NVIDIA RTX 4070+ recommand√© (RTX 3060 minimum).
- **Stockage :** SSD 1 To recommand√© (500 Go minimum).

---

## Logiciels {.subslide}
üß© **Outils n√©cessaires :**
- **Syst√®me :** Windows 10/11, macOS, ou Linux.
- **Python :** 3.9 ou sup√©rieur.
- **Ollama :** ex√©cuter des mod√®les de langage en local.
- **Biblioth√®ques :** LangChain, FastAPI (facultatives selon votre usage).

---

## Les 5 grandes √©tapes
1. <span class="badge">1</span> **D√©finir votre besoin :** pr√©ciser le probl√®me √† r√©soudre.
2. <span class="badge">2</span> **Pr√©parer vos donn√©es :** collecter, nettoyer, structurer.
3. <span class="badge">3</span> **RAG et Fine‚Äëtuning (ajustement fin du mod√®le) :** choisir l‚Äôapproche.
4. <span class="badge">4</span> **Installation compl√®te :** mettre en place l‚Äôenvironnement.
5. <span class="badge">5</span> **Cr√©er votre syst√®me RAG :** tester et it√©rer.

---

## √âtape 1 ‚Äî D√©finir votre besoin
Posez‚Äëvous les bonnes questions :
- Quel probl√®me l‚ÄôIA doit‚Äëelle r√©soudre ?
- √Ä qui s‚Äôadresse‚Äët‚Äëelle et dans quel contexte d‚Äôusage ?
- Quel r√©sultat attendez‚Äëvous (r√©ponse, r√©sum√©, classement, g√©n√©ration de texte) ?

Exemples d‚Äôusages : **recherche documentaire**, **FAQ interne**, **r√©sum√© de rapports**.

---

## √âtape 2 ‚Äî Pr√©parer vos donn√©es
La qualit√© des donn√©es d√©termine la qualit√© des r√©ponses.

**√âtapes simples :**
- **Collecte :** rassembler les documents utiles (PDF, DOCX, TXT).
- **Nettoyage :** supprimer le bruit (balises, urls, artefacts).
- **Organisation :** structurer les textes (titres, sections).

üí° Un petit script peut automatiser ces op√©rations.

---

## Diagramme RAG (Mermaid) {.subslide}
```mermaid
%%{init: {'theme': 'default', 'securityLevel': 'loose'}}%%
graph TD
  A[Question utilisateur] --> B[Recherche de documents]
  B --> C[Base vectorielle]
  C --> D[Construction du contexte]
  D --> E[R√©ponse du mod√®le]
```
---

## Pseudo‚Äëcode RAG {.subslide}
```python
# Pseudo-code RAG (retrieval-augmented generation)
question = "Quelle est la capitale de la France ?"
documents = vector_store.retrieve(question)   # Recherche
contexte = combine(documents)                 # Construit le contexte
reponse = llm.generate(question, contexte)    # G√©n√©ration
print(reponse)
```

> ‚ÑπÔ∏è **Fine‚Äëtuning (ajustement fin du mod√®le)** : √† envisager plus tard, quand RAG seul ne suffit pas.

---

## Installation Ollama {.subslide}
‚öôÔ∏è T√©l√©chargez et installez depuis **https://ollama.com**

```bash
# T√©l√©charger un mod√®le (ex : Llama 3)
ollama pull llama3

# Tester le mod√®le
ollama run llama3 "Bonjour !"
```

---

## Installation Python et d√©pendances {.subslide}
üß© V√©rifier Python et installer quelques biblioth√®ques utiles :

```bash
python3 --version

pip install langchain ollama fastapi uvicorn
```

---

## V√©rification & choix du mod√®le
üß† Test rapide pour v√©rifier que tout fonctionne :

```python
from langchain_community.llms import Ollama
llm = Ollama(model="llama3")
print(llm.invoke("Salut !"))  # Doit r√©pondre
```

**Choisir le mod√®le :** adaptez‚Äële √† votre mat√©riel (ex. *Llama 3 8B* ‚âà 8 Go VRAM).

---

## Cr√©er votre premier RAG
Chemin simple en 6 √©tapes :
1) Charger un ou deux fichiers texte  
2) D√©couper en petits morceaux (chunks)  
3) Cr√©er des **embeddings** et une base vectorielle  
4) Configurer le mod√®le (Ollama)  
5) Poser une question ‚Üí r√©cup√©rer et injecter le contexte  
6) Afficher la r√©ponse

```python
# Exemple minimal illustratif
documents = load_texts(["notes.txt"])
chunks = split(documents, size=800, overlap=150)
vs = build_vector_store(chunks)     # embeddings + index
llm = local_llm("llama3")
print(ask(llm, "Sujet du document ?", vs))
```

---

## Exemple concret ‚Äî Assistant de cours
üéì √âtudiant face √† un m√©moire de 350 pages.

**Avant :** heures de recherche manuelle.  
**Apr√®s :** quelques minutes pour obtenir des r√©ponses sourc√©es.  
**B√©n√©fice :** concentration sur l‚Äôessentiel.

---

## Probl√®mes courants & optimisations
‚ö†Ô∏è **Probl√®mes fr√©quents**
- Pilotes GPU non √† jour / VRAM insuffisante
- Mod√®le trop gros ‚Üí lenteur
- Donn√©es brutes ou mal segment√©es

üí° **Astuces**
- Ajuster la taille/recouvrement des morceaux
- Pr√©parer/filtrer les sources
- Commencer petit, it√©rer souvent

---

## IA Locale {.subslide}
üè† **Atouts**
- Confidentialit√© maximale
- Co√ªts ma√Ætris√©s (pas d‚Äôabonnement)
- Ind√©pendance (hors‚Äëligne possible)

**Limites**
- D√©pend du mat√©riel disponible
- Un peu de prise en main au d√©part

---

## IA Cloud {.subslide}
‚òÅÔ∏è **Atouts**
- Scalabilit√© et simplicit√© de d√©ploiement
- Acc√®s aux tout derniers mod√®les

**Limites**
- Co√ªts r√©currents
- D√©pendance √† un fournisseur
- Questions de confidentialit√©

---

# Conclusion ‚Äî Lancez‚Äëvous !
L‚ÄôIA locale est **accessible et utile**. Commencez petit, exp√©rimentez et progressez.

üöÄ *√Ä vous de jouer !*

---

## Merci ! Des questions ?
Contact : votre.email@example.com
